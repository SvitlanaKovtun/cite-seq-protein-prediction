{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aa64508-f306-4c56-8489-b0667b36a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89018\n"
     ]
    }
   ],
   "source": [
    "# (scratch) runtime / memory check — safe to delete before publishing\n",
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "689bfdb9-219a-4696-8ff9-c6fc9a4184f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Limit threads for numerical libraries to manage CPU usage\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"7\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"7\"\n",
    "\n",
    "base_dir = \"/home/skovtun/Python_projects/Kaggle/Single_cell/\"\n",
    "data_dir = os.path.join(base_dir, \"data\")\n",
    "random_state = 77\n",
    "\n",
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045f2f59-1df2-423c-99fd-ab2d8618cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing Day 2 issues in metadata file\n",
    "metadata_old = pd.read_csv('metadata.csv', index_col = 'cell_id')\n",
    "fix = pd.read_csv('metadata_cite_day_2_donor_27678.csv', index_col = 'cell_id')\n",
    "metadata = pd.concat([metadata_old, fix], axis = 0)\n",
    "del fix, metadata_old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362b376-1be2-4994-8d7d-d047400e0834",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Appendix: archived cell-type–specific experiments (not used in final pipeline)\n",
    "\n",
    "#splitting cite test to 7 groups based on cell type.\n",
    "# fix has more columns than initial test file, so I am cutting the columns so the datasets \n",
    "#whoud be aligned\n",
    "# import time\n",
    "# evaluation_old = pd.read_hdf('Single_cell_data/test_cite_inputs.h5')\n",
    "# fix = pd.read_hdf('Single_cell_data/test_cite_inputs_day_2_donor_27678.h5')\n",
    "# common_cols = evaluation_old.columns\n",
    "# fix_aligned = fix[common_cols]\n",
    "# cite_evaluation = pd.concat([evaluation_old,fix_aligned], axis = 0)\n",
    "# del evaluation_old, fix\n",
    "# cite_metadata = metadata[metadata['technology'] == 'citeseq']\n",
    "# cell_types = cite_metadata['cell_type'].unique()\n",
    "# base_dir = \"/home/skovtun/Python_projects/Kaggle/Single cell\"\n",
    "# data_dir = os.path.join(base_dir, \"Single_cell_data\")\n",
    "\n",
    "# os.chdir(data_dir)\n",
    "# for cell_type in cell_types:\n",
    "#     print(f\"Processing {cell_type}...\")\n",
    "#     start_time = time.time()\n",
    "#     idx = cite_metadata[cite_metadata['cell_type'] == cell_type].index\n",
    "#     valid_ids = idx.intersection(cite_evaluation.index)\n",
    "#     subset = cite_evaluation.loc[valid_ids]\n",
    "\n",
    "#     # write directly to file under its own key\n",
    "#     subset.to_parquet(\n",
    "#     f\"X_eval{cell_type}.parquet\",\n",
    "#     engine=\"pyarrow\",\n",
    "#     index=True,\n",
    "#     coerce_timestamps=\"ms\",\n",
    "#     allow_truncated_timestamps=True,\n",
    "#     version=\"2.6\")\n",
    "#     print((time.time() - start_time)/60)\n",
    "#     # free memory immediately\n",
    "#     del subset\n",
    "\n",
    "# os.chdir(base_dir)\n",
    "# import gc; gc.collect()\n",
    "\n",
    "# os.chdir(data_dir)\n",
    "# targets = pd.read_hdf('train_cite_targets.h5') #all type of cells\n",
    "# inputs1 = pd.read_parquet('X_HSC.parquet')\n",
    "# inputs2 = pd.read_parquet('X_MasP.parquet')\n",
    "# inputs3 = pd.read_parquet('X_MkP.parquet')\n",
    "# inputs4 = pd.read_parquet('X_MoP.parquet')\n",
    "# inputs5 = pd.read_parquet('X_NeuP.parquet')\n",
    "# inputs6 = pd.read_parquet('X_BP.parquet')\n",
    "# inputs7 = pd.read_parquet('X_EryP.parquet')\n",
    "\n",
    "# inputs = pd.concat([inputs1,inputs2,inputs3,inputs4,inputs5,inputs6, inputs7], axis = 0)\n",
    "# del inputs1,inputs2,inputs3,inputs4,inputs5,inputs6,inputs7\n",
    "\n",
    "# evaluation1 = pd.read_parquet('X_evalHSC.parquet')\n",
    "# evaluation2 = pd.read_parquet('X_evalMasP.parquet')\n",
    "# evaluation3 = pd.read_parquet('X_evalMkP.parquet')\n",
    "# evaluation4 = pd.read_parquet('X_evalMoP.parquet')\n",
    "# evaluation5 = pd.read_parquet('X_evalNeuP.parquet')\n",
    "# evaluation6 = pd.read_parquet('X_evalBP.parquet')\n",
    "# evaluation7 = pd.read_parquet('X_evalEryP.parquet')\n",
    "# evaluation = pd.concat([evaluation1,evaluation2,evaluation3,evaluation4,evaluation5,evaluation6,evaluation7], axis=0)\n",
    "\n",
    "# del evaluation1,evaluation2,evaluation3,evaluation4,evaluation5,evaluation6,evaluation7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7a03cc-f662-4094-bb2b-b1a6e6dcbb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CITEseq training data...\n",
      "Loading CITEseq evaluation data and applying fix...\n",
      "Data loaded. Train shape: (70988, 22050), Evaluation shape: (55679, 22050)\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Process Training Data ---\n",
    "\n",
    "print(\"Loading CITEseq training data...\")\n",
    "\n",
    "# 1. Load the main training inputs\n",
    "cite_train = pd.read_hdf(os.path.join(data_dir, 'train_cite_inputs.h5'))\n",
    "\n",
    "# 2. Filter for specific cell types defined in metadata\n",
    "cite_metadata = metadata[metadata['technology'] == 'citeseq']\n",
    "\n",
    "# Intersection ensures we only get IDs that exist in both metadata and input data\n",
    "valid_ids = (cite_metadata.index).intersection(cite_train.index)\n",
    "cite_train = cite_train.loc[valid_ids]\n",
    "\n",
    "# --- Load and Process Evaluation Data ---\n",
    "# The evaluation set (test) also requires the Day 2 donor fix\n",
    "\n",
    "print(\"Loading CITEseq evaluation data and applying fix...\")\n",
    "evaluation_base = pd.read_hdf(os.path.join(data_dir, 'test_cite_inputs.h5'))\n",
    "evaluation_fix = pd.read_hdf(os.path.join(data_dir, 'test_cite_inputs_day_2_donor_27678.h5'))\n",
    "\n",
    "# Align columns: The fix has extra columns, so we align it to the base\n",
    "common_cols = evaluation_base.columns\n",
    "evaluation_fix = evaluation_fix[common_cols]\n",
    "\n",
    "# Combine base evaluation data with the fix\n",
    "cite_evaluation = pd.concat([evaluation_base, evaluation_fix], axis=0)\n",
    "\n",
    "# Filter evaluation data for the same target cell types\n",
    "valid_eval_ids = (cite_metadata.index).intersection(cite_evaluation.index)\n",
    "cite_evaluation = cite_evaluation.loc[valid_eval_ids]\n",
    "\n",
    "# Clean up memory\n",
    "del evaluation_base, evaluation_fix\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Data loaded. Train shape: {cite_train.shape}, Evaluation shape: {cite_evaluation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92c9970-d720-4b03-b138-979546353df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directly relevant features (preserved): 103\n",
      "Features to compress via PCA: 21947\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Selection Strategy ---\n",
    "# Goal: Retain raw features for genes that directly code for the target proteins (Surface Markers).\n",
    "# All other genes will be compressed using PCA to reduce dimensionality while preserving variance.\n",
    "\n",
    "# 1. Load Targets\n",
    "targets = pd.read_hdf(os.path.join(data_dir, 'train_cite_targets.h5'))\n",
    "\n",
    "# 2. Identify \"Important\" Genes using Barcode Lookup\n",
    "# We use an external file (TotalSeq antibody reference) \n",
    "# from https://www.biolegend.com/en-us/totalseq/barcode-lookup \n",
    "# to map protein targets to their corresponding gene IDs.\n",
    "barcode_path = os.path.join(data_dir, 'Totalseq_a.csv')\n",
    "barcode = pd.read_csv(barcode_path)\n",
    "\n",
    "proteins = targets.columns\n",
    "all_protein_barcodes = barcode[barcode['Description'].isin(proteins)].sort_values('Description')\n",
    "\n",
    "# Filter for Human reactivity to exclude mouse controls\n",
    "h_protein_barcodes = all_protein_barcodes[all_protein_barcodes['Reactivity'].str.contains('Human')]\n",
    "ens_gene_ids = list(h_protein_barcodes['Ensembl Gene Id'].unique())\n",
    "\n",
    "del barcode, all_protein_barcodes, h_protein_barcodes\n",
    "gc.collect()\n",
    "\n",
    "# 3. Clean Column Names (Remove suffixes to match gene IDs)\n",
    "clean_input_cols = [c.split('_')[0] for c in cite_train.columns]\n",
    "eval_input_cols = [c.split('_')[0] for c in cite_evaluation.columns]\n",
    "\n",
    "# Verify consistency between train and test sets\n",
    "assert clean_input_cols == eval_input_cols\n",
    "\n",
    "cite_train.columns = clean_input_cols\n",
    "cite_evaluation.columns = eval_input_cols\n",
    "\n",
    "# 4. Split Features into \"Important\" (keep raw) vs \"Background\" (PCA)\n",
    "common_cols = list(set(ens_gene_ids) & set(cite_train.columns))\n",
    "pca_cols = list(set(cite_train.columns) - set(common_cols))\n",
    "\n",
    "print(f\"Directly relevant features (preserved): {len(common_cols)}\")\n",
    "print(f\"Features to compress via PCA: {len(pca_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37bfe2d-2afb-4940-851c-b3887045ec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into Train/Test and separating features...\n",
      "Train samples: 56790, Test samples: 14198\n"
     ]
    }
   ],
   "source": [
    "# --- Data Splitting and Feature Separation ---\n",
    "\n",
    "# 1. Align Inputs and Targets\n",
    "# Ensure we only use rows where we have both input data and target values\n",
    "valid_ids = cite_train.index.intersection(targets.index)\n",
    "X_subset = cite_train.loc[valid_ids]\n",
    "y_subset = targets.loc[valid_ids]\n",
    "\n",
    "# 2. Random Train-Test Split (80/20)\n",
    "rng = np.random.default_rng(random_state)\n",
    "shuffled_idx = rng.permutation(valid_ids)\n",
    "\n",
    "split_point = int(len(shuffled_idx) * 0.8)\n",
    "train_idx = shuffled_idx[:split_point]\n",
    "test_idx = shuffled_idx[split_point:]\n",
    "\n",
    "# 3. Create Subsets and Split Features\n",
    "# We split both Train and Test data into two parts:\n",
    "#   A) _orig: Features to keep as-is (biologically relevant)\n",
    "#   B) _compress: Features to reduce via PCA (high-dimensional background)\n",
    "\n",
    "print(\"Splitting data into Train/Test and separating features...\")\n",
    "\n",
    "# Train sets\n",
    "inputs_train = X_subset.loc[train_idx]\n",
    "inputs_train_orig = inputs_train[common_cols]\n",
    "inputs_train_to_compress = inputs_train[pca_cols]\n",
    "\n",
    "# Test sets\n",
    "inputs_test = X_subset.loc[test_idx]\n",
    "inputs_test_orig = inputs_test[common_cols]\n",
    "inputs_test_to_compress = inputs_test[pca_cols]\n",
    "\n",
    "# Targets\n",
    "y_train, y_test = y_subset.loc[train_idx], y_subset.loc[test_idx]\n",
    "\n",
    "# Evaluation sets (Final submission data)\n",
    "# We apply the same feature separation to the evaluation set\n",
    "eval_orig = cite_evaluation[common_cols]\n",
    "eval_to_compress = cite_evaluation[pca_cols]\n",
    "\n",
    "# Clean up large objects to free memory\n",
    "del cite_train, cite_evaluation, X_subset, y_subset, inputs_train, inputs_test, targets\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Train samples: {len(train_idx)}, Test samples: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d0ed28-e468-413d-ac1e-ca05ea9d6c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Scaling and Dimensionality Reduction (PCA) ---\n",
    "\n",
    "# 1. Standard Scaling (on background genes only)\n",
    "# Fit ONLY on training data to prevent data leakage\n",
    "scaler = StandardScaler()\n",
    "\n",
    "inputs_train_scaled = scaler.fit_transform(inputs_train_to_compress)\n",
    "inputs_test_scaled = scaler.transform(inputs_test_to_compress)\n",
    "evaluation_scaled = scaler.transform(eval_to_compress)\n",
    "\n",
    "# Free memory\n",
    "del inputs_train_to_compress, inputs_test_to_compress, eval_to_compress\n",
    "gc.collect()\n",
    "\n",
    "# 2. PCA Compression\n",
    "# Reducing background transcriptome to 200 latent featurespca = PCA(n_components=200, random_state=random_state)\n",
    "pca = PCA(n_components=200)\n",
    "inputs_train_pca = pca.fit_transform(inputs_train_scaled)\n",
    "inputs_test_pca = pca.transform(inputs_test_scaled)\n",
    "evaluation_pca = pca.transform(evaluation_scaled)\n",
    "\n",
    "# Free memory\n",
    "del inputs_train_scaled, inputs_test_scaled, evaluation_scaled\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3060143-a1ea-4f57-9f32-74cf6137a5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature count: 303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1042"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Feature Combination ---\n",
    "# Mixing original \"important\" genes with PCA-transformed background features.\n",
    "# Note: Dealing with this mixture makes it difficult to simply regress out the donor effect \n",
    "# at this stage because the signal is very strong and complex (linear/non-linear).\n",
    "\n",
    "# 1. Horizontal Stack \n",
    "inputs_train_all = np.hstack([inputs_train_orig.values, inputs_train_pca])\n",
    "inputs_test_all = np.hstack([inputs_test_orig.values, inputs_test_pca])\n",
    "evaluation_all = np.hstack([eval_orig.values, evaluation_pca])\n",
    "\n",
    "# 2. Create DataFrames with Column Names\n",
    "pca_col_names = [f'PC{i+1}' for i in range(inputs_train_pca.shape[1])]\n",
    "all_cols = list(inputs_train_orig.columns) + pca_col_names\n",
    "\n",
    "print(f\"Total feature count: {len(all_cols)}\")\n",
    "\n",
    "X_train_all = pd.DataFrame(inputs_train_all, index=train_idx, columns=all_cols)\n",
    "X_test_all = pd.DataFrame(inputs_test_all, index=test_idx, columns=all_cols)\n",
    "X_eval_all = pd.DataFrame(evaluation_all, index=valid_eval_ids, columns=all_cols)\n",
    "\n",
    "# Free up numpy arrays to save memory\n",
    "del inputs_train_all, inputs_test_all, evaluation_all, inputs_train_orig, inputs_train_pca\n",
    "del inputs_test_orig, inputs_test_pca, eval_orig, evaluation_pca\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3179b8-1bac-4995-af82-35c82772f0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata merged.\n",
      "X_train shape: (56790, 306)\n"
     ]
    }
   ],
   "source": [
    "# --- Metadata Enrichment ---\n",
    "# Merging the biological metadata (day, donor, cell type) into our feature sets.\n",
    "# We perform a left join on the index (cell_id) to ensure alignment.\n",
    "\n",
    "# 1. Define metadata columns to keep\n",
    "meta_cols = ['day', 'donor', 'cell_type']\n",
    "\n",
    "# 2. Merge metadata\n",
    "# Note: X_train_all, X_test_all, X_eval_all are the dataframes created in the previous step\n",
    "X_train = pd.merge(X_train_all, metadata[meta_cols], how='left', left_index=True, right_index=True)\n",
    "X_test = pd.merge(X_test_all, metadata[meta_cols], how='left', left_index=True, right_index=True)\n",
    "X_eval = pd.merge(X_eval_all, metadata[meta_cols], how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Free intermediate memory\n",
    "del X_train_all, X_test_all, X_eval_all, metadata\n",
    "gc.collect()\n",
    "\n",
    "print(\"Metadata merged.\")\n",
    "print(f\"X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a937878-387d-40e6-b5cc-fed90439d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding complete.\n",
      "Current feature shape: (56790, 312)\n"
     ]
    }
   ],
   "source": [
    "# --- Categorical Encoding ---\n",
    "# Converting 'cell_type' into One-Hot Encoded features.\n",
    "# This allows the model to explicitly use cell identity (e.g., HSC vs EryP) as a predictor.\n",
    "\n",
    "# 1. Ensure categorical data type (optimizes memory and speed)\n",
    "X_train['cell_type'] = X_train['cell_type'].astype('category')\n",
    "X_test['cell_type']  = X_test['cell_type'].astype('category') \n",
    "X_eval['cell_type']  = X_eval['cell_type'].astype('category') \n",
    "\n",
    "# 2. Apply One-Hot Encoding\n",
    "# We use the 'ct' prefix (e.g., 'ct_HSC') to make these features easy to track\n",
    "X_train = pd.get_dummies(X_train, columns=['cell_type'], prefix='ct', dummy_na=False)\n",
    "X_test  = pd.get_dummies(X_test,  columns=['cell_type'], prefix='ct', dummy_na=False)\n",
    "X_eval  = pd.get_dummies(X_eval,  columns=['cell_type'], prefix='ct', dummy_na=False)\n",
    "\n",
    "print(\"One-Hot Encoding complete.\")\n",
    "print(f\"Current feature shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d5356eb-7325-48ff-9966-35499d0a68d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing protein modeling difficulty (Pearson)...\n",
      "Selected 12 proteins spanning biological & technical diversity.\n",
      "['CD194', 'CD196', 'CD105', 'CD278', 'CD335', 'CD141', 'IgD', 'CD1c', 'CD19', 'CD86', 'CD155', 'CD41']\n"
     ]
    }
   ],
   "source": [
    "# --- Representative Protein Selection (Optimal Grid Strategy) ---\n",
    "# Strategy: Select ~12 proteins that span the full \"Signal vs. Difficulty\" space.\n",
    "# Axis 1: Modeling Difficulty (Ridge Baseline Pearson)\n",
    "# Axis 2: Signal Strength (Variance)\n",
    "print(\"Assessing protein modeling difficulty (Pearson)...\")\n",
    "\n",
    "# 1. Define Custom Scorer (Pearson Correlation)\n",
    "def pearson_score(y_true, y_pred):\n",
    "    # Handle constant predictions to avoid division by zero\n",
    "    if np.std(y_pred) < 1e-9:\n",
    "        return 0.0\n",
    "    return np.corrcoef(y_true, y_pred)[0, 1]\n",
    "\n",
    "pearson_scorer = make_scorer(pearson_score)\n",
    "\n",
    "# 2. Calculate Metrics for All Proteins\n",
    "protein_names = y_train.columns\n",
    "stats = []\n",
    "ridge = Ridge(alpha=1.0) \n",
    "\n",
    "for protein in protein_names:\n",
    "    y = y_train[protein].values\n",
    "    \n",
    "    # Axis 1: Difficulty (Baseline Model Performance)\n",
    "    score = cross_val_score(\n",
    "        ridge, X_train, y_train[protein],\n",
    "        cv=3,\n",
    "        scoring=pearson_scorer, \n",
    "        n_jobs=-1\n",
    "    ).mean()\n",
    "    \n",
    "    # Axis 2: Signal Strength (Variance)\n",
    "    var = np.var(y)\n",
    "    \n",
    "    stats.append({\"protein\": protein, \"var\": var, \"score\": score})\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "\n",
    "# 3. Define Quadrants (2x2 Grid)\n",
    "stats_df[\"var_bin\"] = pd.qcut(stats_df[\"var\"], 2, labels=[\"Low Signal\", \"High Signal\"])\n",
    "stats_df[\"score_bin\"] = pd.qcut(stats_df[\"score\"], 2, labels=[\"Hard\", \"Easy\"])\n",
    "\n",
    "# 4. Select 3 Proteins per Quadrant (Min, Median, Max)\n",
    "rep_proteins = []\n",
    "\n",
    "for v in [\"Low Signal\", \"High Signal\"]:\n",
    "    for r in [\"Hard\", \"Easy\"]:\n",
    "        block = stats_df[(stats_df[\"var_bin\"] == v) & (stats_df[\"score_bin\"] == r)]\n",
    "        \n",
    "        if len(block) == 0: continue\n",
    "            \n",
    "        # A) The \"Worst\" in class (Boundary Test)\n",
    "        rep_proteins.append(block.iloc[block[\"score\"].argmin()][\"protein\"])\n",
    "        \n",
    "        # B) The \"Average\" in class (Typical Case)\n",
    "        median_idx = (block[\"score\"] - block[\"score\"].median()).abs().argmin()\n",
    "        rep_proteins.append(block.iloc[median_idx][\"protein\"])\n",
    "\n",
    "        # C) The \"Best\" in class (Boundary Test)\n",
    "        rep_proteins.append(block.iloc[block[\"score\"].argmax()][\"protein\"])\n",
    "\n",
    "# Cleanup\n",
    "rep_proteins = list(dict.fromkeys(rep_proteins))\n",
    "print(f\"Selected {len(rep_proteins)} proteins spanning biological & technical diversity.\")\n",
    "print(rep_proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c0f007d-6b8b-4aab-8d9a-1d8d1b746ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 14 proteins for tuning.\n",
      "Includes manual overrides: ['Mouse-IgG2a', 'CD22']\n",
      "['CD194', 'CD196', 'CD105', 'CD278', 'CD335', 'CD141', 'IgD', 'CD1c', 'CD19', 'CD86', 'CD155', 'CD41', 'Mouse-IgG2a', 'CD22']\n"
     ]
    }
   ],
   "source": [
    "# 2. Manual Override: Force-include specific targets for safety:\n",
    "#    - 'Mouse-IgG2a': Isotype control (sanity check for overfitting).\n",
    "#    - 'CD22': B-cell marker to ensure coverage of the rare 'BP' cell type.\n",
    "manual_adds = ['Mouse-IgG2a', 'CD22']\n",
    "rep_proteins.extend(manual_adds)\n",
    "\n",
    "# Cleanup\n",
    "rep_proteins = list(dict.fromkeys(rep_proteins))\n",
    "\n",
    "# Filter to ensure they actually exist in the data (just in case)\n",
    "rep_proteins = [p for p in rep_proteins if p in y_train.columns]\n",
    "\n",
    "print(f\"Selected {len(rep_proteins)} proteins for tuning.\")\n",
    "print(f\"Includes manual overrides: {manual_adds}\")\n",
    "print(rep_proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24fe936b-3bad-46c5-b17f-1d6cdc2e7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost Helper Functions ---\n",
    "\n",
    "def kaggle_mean_cellwise_pearson(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mean Pearson correlation per cell (row-wise).\n",
    "    Mimics the competition metric. Constant predictions yield -1.\n",
    "    \"\"\"\n",
    "    yt = np.asarray(y_true)\n",
    "    yp = np.asarray(y_pred)\n",
    "    \n",
    "    # Center data (subtract mean)\n",
    "    y_pred_centered = yp - yp.mean(axis=1, keepdims=True)\n",
    "    y_true_centered = yt - yt.mean(axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate numerator and denominator\n",
    "    num = np.sum(y_true_centered * y_pred_centered, axis=1)\n",
    "    den = np.sqrt(np.sum(y_true_centered ** 2, axis=1) * np.sum(y_pred_centered ** 2, axis=1))\n",
    "    \n",
    "    # Handle constant predictions (zero variance)\n",
    "    const_pred = np.all(yp == yp[:, [0]], axis=1)\n",
    "    \n",
    "    corrs = np.empty(y_true.shape[0], dtype=float)\n",
    "    corrs[const_pred] = -1.0  # Penalize constant predictions\n",
    "    \n",
    "    # Calculate valid correlations\n",
    "    valid = (~const_pred) & (den > 0)\n",
    "    corrs[valid] = num[valid] / den[valid]\n",
    "    \n",
    "    # Handle remaining edge cases\n",
    "    corrs[~const_pred & (den == 0)] = np.nan\n",
    "    \n",
    "    return np.nanmean(corrs)\n",
    "\n",
    "def split_indices_donor(X_df, donor_col=\"donor\", random_state=77):\n",
    "    \"\"\"\n",
    "    Splits data by holding out ONE donor for validation.\n",
    "    Crucial for testing generalization to new batches/donors.\n",
    "    \"\"\"\n",
    "    donors = X_df[donor_col].unique()\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    val_donor = rng.choice(donors)\n",
    "    \n",
    "    train_idx = np.where(X_df[donor_col] != val_donor)[0]\n",
    "    val_idx   = np.where(X_df[donor_col] == val_donor)[0]\n",
    "    \n",
    "    return train_idx, val_idx, val_donor\n",
    "\n",
    "def to_float32(df):\n",
    "    \"\"\"Converts DataFrame to float32 numpy array (XGBoost friendly).\"\"\"\n",
    "    return df.to_numpy().astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e2b676-a8b7-4e7b-970a-da96d5c535e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Donor: 31800\n",
      "Train size: 36965 | Validation size: 19825\n",
      "\n",
      "Starting tuning on 14 representative proteins...\n",
      "[1/4] Score: 0.81211 | Params: {'max_depth': 5, 'min_child_weight': 40, 'subsample': 0.88, 'colsample_bytree': 0.4, 'learning_rate': 0.05, 'reg_alpha': 0.3, 'reg_lambda': 4.0, 'gamma': 0.0}\n",
      "[2/4] Score: 0.81235 | Params: {'max_depth': 6, 'min_child_weight': 30, 'subsample': 0.85, 'colsample_bytree': 0.4, 'learning_rate': 0.05, 'reg_alpha': 0.1, 'reg_lambda': 15.0, 'gamma': 0.0}\n",
      "[3/4] Score: 0.81055 | Params: {'max_depth': 4, 'min_child_weight': 60, 'subsample': 0.8, 'colsample_bytree': 0.45, 'learning_rate': 0.05, 'reg_alpha': 0.0, 'reg_lambda': 5.0, 'gamma': 0.0}\n",
      "[4/4] Score: 0.81170 | Params: {'max_depth': 5, 'min_child_weight': 40, 'subsample': 0.9, 'colsample_bytree': 0.3, 'learning_rate': 0.05, 'reg_alpha': 0.5, 'reg_lambda': 5.0, 'gamma': 0.1}\n",
      "\n",
      "--- Best Configuration ---\n",
      "Score: 0.81235\n",
      "{'max_depth': 6, 'min_child_weight': 30, 'subsample': 0.85, 'colsample_bytree': 0.4, 'learning_rate': 0.05, 'reg_alpha': 0.1, 'reg_lambda': 15.0, 'gamma': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# --- XGBoost Hyperparameter Tuning (Final Champion vs. Challengers) ---\n",
    "# Strategy: My previous \"Winner\" against 3 distinct challengers.\n",
    "# Goal: See if the new protein list requires a shift in strategy.\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# 1. Define the Candidate Grid\n",
    "candidate_params = [\n",
    "    # Candidate A: \"The Champion\" (previous winner)\n",
    "    # L1 regularization (alpha=0.3) helps sparse feature selection.\n",
    "    dict(max_depth=5, min_child_weight=40, subsample=0.88, colsample_bytree=0.4,\n",
    "         learning_rate=0.05, reg_alpha=0.3, reg_lambda=4.0, gamma=0.0),\n",
    "\n",
    "    # Candidate B: \"The Deep & Rigid\" (Challenger)\n",
    "    # Deeper trees (6) to catch complex interactions, but heavily clamped (Lambda=15)\n",
    "    # to prevent overfitting. Good if the Champion is underfitting.\n",
    "    dict(max_depth=6, min_child_weight=30, subsample=0.85, colsample_bytree=0.4,\n",
    "         learning_rate=0.05, reg_alpha=0.1, reg_lambda=15.0, gamma=0.0),\n",
    "\n",
    "    # Candidate C: \"The Conservative\" (Challenger)\n",
    "    # Shallower (4) and higher smoothing (60). \n",
    "    # Best if the new protein list (e.g. CD22, IgG2a) is noisier than before.\n",
    "    dict(max_depth=4, min_child_weight=60, subsample=0.80, colsample_bytree=0.45,\n",
    "         learning_rate=0.05, reg_alpha=0.0, reg_lambda=5.0, gamma=0.0),\n",
    "\n",
    "    # Candidate D: \"The Feature Bagger\" (Challenger)\n",
    "    # Aggressive feature sampling (0.3) and high subsample (0.9).\n",
    "    # Forces diversity between trees. Good for highly correlated gene data.\n",
    "    dict(max_depth=5, min_child_weight=40, subsample=0.9, colsample_bytree=0.3,\n",
    "         learning_rate=0.05, reg_alpha=0.5, reg_lambda=5.0, gamma=0.1),\n",
    "]\n",
    "\n",
    "# 2. Prepare Data (Split by Donor)\n",
    "# Note: Using the helper function defined earlier\n",
    "tr_idx, va_idx, val_donor = split_indices_donor(X_train, donor_col=\"donor\", random_state=77)\n",
    "\n",
    "print(f\"Validation Donor: {val_donor}\")\n",
    "print(f\"Train size: {len(tr_idx)} | Validation size: {len(va_idx)}\")\n",
    "\n",
    "# Drop metadata columns ('day', 'donor') before converting\n",
    "X_feat = X_train.drop(columns=['day', 'donor']) \n",
    "X_tr = to_float32(X_feat.iloc[tr_idx])\n",
    "X_va = to_float32(X_feat.iloc[va_idx])\n",
    "\n",
    "# Select only the chosen representative proteins\n",
    "Y_tr = y_train.iloc[tr_idx][rep_proteins].to_numpy(dtype=np.float32)\n",
    "Y_va = y_train.iloc[va_idx][rep_proteins].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Create DMatrices once\n",
    "dtrain = xgb.DMatrix(X_tr)\n",
    "dvalid = xgb.DMatrix(X_va)\n",
    "\n",
    "# 3. The Tuning Loop\n",
    "scores = []\n",
    "\n",
    "print(f\"\\nStarting tuning on {len(rep_proteins)} representative proteins...\")\n",
    "\n",
    "for k, cand in enumerate(candidate_params, 1):\n",
    "    # Merge candidate params with fixed params\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\", \n",
    "        \"tree_method\": \"hist\",\n",
    "        \"seed\": 77,\n",
    "        \"verbosity\": 0,\n",
    "        **cand\n",
    "    }\n",
    "    \n",
    "    preds_accum = np.zeros_like(Y_va, dtype=np.float32)\n",
    "\n",
    "    # Train a model for each representative protein\n",
    "    for j, protein_name in enumerate(rep_proteins):\n",
    "        dtrain.set_label(Y_tr[:, j])\n",
    "        dvalid.set_label(Y_va[:, j])\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=5000, \n",
    "            evals=[(dvalid, \"valid\")],\n",
    "            early_stopping_rounds=100, \n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        preds_accum[:, j] = booster.predict(dvalid, iteration_range=(0, booster.best_iteration + 1))\n",
    "\n",
    "    # Score: Row-wise Pearson (Competition Metric)\n",
    "    score = kaggle_mean_cellwise_pearson(Y_va, preds_accum)\n",
    "    scores.append({\"cand\": cand, \"score\": score})\n",
    "\n",
    "    print(f\"[{k}/{len(candidate_params)}] Score: {score:.5f} | Params: {cand}\")\n",
    "\n",
    "# 4. Select Best\n",
    "best_result = max(scores, key=lambda x: x[\"score\"])\n",
    "best_params = best_result[\"cand\"]\n",
    "\n",
    "print(\"\\n--- Best Configuration ---\")\n",
    "print(f\"Score: {best_result['score']:.5f}\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ade1da47-313d-4853-8ac3-0cfa021ae0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting production training...\n",
      "Training on 140 proteins...\n",
      "Feature set includes 'day'.\n",
      "Processed 10/140 proteins\n",
      "Processed 20/140 proteins\n",
      "Processed 30/140 proteins\n",
      "Processed 40/140 proteins\n",
      "Processed 50/140 proteins\n",
      "Processed 60/140 proteins\n",
      "Processed 70/140 proteins\n",
      "Processed 80/140 proteins\n",
      "Processed 90/140 proteins\n",
      "Processed 100/140 proteins\n",
      "Processed 110/140 proteins\n",
      "Processed 120/140 proteins\n",
      "Processed 130/140 proteins\n",
      "Processed 140/140 proteins\n",
      "\n",
      "--- Model Diagnostics ---\n",
      "Train Score (Pearson): 0.94234\n",
      "Test Score  (Pearson): 0.90156\n",
      "Generalization Gap:    0.04077\n",
      "SUCCESS: Model is generalizing well.\n"
     ]
    }
   ],
   "source": [
    "# --- Production Training & Diagnostics ---\n",
    "# Training final models on the full dataset using the winning \"Deep & Rigid\" hyperparameters.\n",
    "# We predict on BOTH Train and Test sets to measure the \"Generalization Gap\" (Overfitting).\n",
    "\n",
    "print(\"Starting production training...\")\n",
    "\n",
    "# 1. Setup Data\n",
    "# Keep 'day' (differentiation time), drop 'donor' (batch effect).\n",
    "X_tr_full = to_float32(X_train.drop(columns=['donor']))\n",
    "X_te_full = to_float32(X_test.drop(columns=['donor']))\n",
    "\n",
    "dtrain_full = xgb.DMatrix(X_tr_full)\n",
    "dtest_full  = xgb.DMatrix(X_te_full)\n",
    "\n",
    "# 2. Output Containers\n",
    "# We need to store Train predictions now too\n",
    "y_pred_train = pd.DataFrame(index=X_train.index, columns=y_train.columns, dtype=np.float32)\n",
    "y_pred_test  = pd.DataFrame(index=X_test.index,  columns=y_train.columns, dtype=np.float32)\n",
    "\n",
    "# 3. Final Parameters (The Winner: Deep & Rigid)\n",
    "final_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"seed\": 77,\n",
    "    \"verbosity\": 0,\n",
    "    \n",
    "    # Winning Settings\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,              \n",
    "    \"min_child_weight\": 30,      \n",
    "    \"subsample\": 0.85, \n",
    "    \"colsample_bytree\": 0.4,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 15.0,          \n",
    "    \"gamma\": 0.0,\n",
    "}\n",
    "\n",
    "# 4. Training Loop (All Proteins)\n",
    "NUM_ROUNDS = 700\n",
    "\n",
    "print(f\"Training on {len(y_train.columns)} proteins...\")\n",
    "\n",
    "\n",
    "for i, protein in enumerate(y_train.columns, 1):\n",
    "    # Prepare target\n",
    "    y_target = y_train[protein].to_numpy(dtype=np.float32)\n",
    "    dtrain_full.set_label(y_target)\n",
    "\n",
    "    # Train\n",
    "    booster = xgb.train(\n",
    "        params=final_params,\n",
    "        dtrain=dtrain_full,\n",
    "        num_boost_round=NUM_ROUNDS,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Predict on Train AND Test to check overfitting\n",
    "    y_pred_train[protein] = booster.predict(dtrain_full)\n",
    "    y_pred_test[protein]  = booster.predict(dtest_full)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i}/{len(y_train.columns)} proteins\")\n",
    "\n",
    "# 5. Diagnostic Evaluation\n",
    "print(\"\\n--- Model Diagnostics ---\")\n",
    "\n",
    "# Calculate scores\n",
    "train_score = kaggle_mean_cellwise_pearson(y_train, y_pred_train)\n",
    "test_score  = kaggle_mean_cellwise_pearson(y_test,  y_pred_test)\n",
    "\n",
    "print(f\"Train Score (Pearson): {train_score:.5f}\")\n",
    "print(f\"Test Score  (Pearson): {test_score:.5f}\")\n",
    "\n",
    "gap = train_score - test_score\n",
    "print(f\"Generalization Gap:    {gap:.5f}\")\n",
    "\n",
    "if gap > 0.05:\n",
    "    print(\"WARNING: Significant overfitting detected (>0.05 gap). Consider increasing reg_lambda.\")\n",
    "else:\n",
    "    print(\"SUCCESS: Model is generalizing well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "861235d6-8b12-48ea-a893-252729cc871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Grand Finale: Retraining on ALL available data (Train + Test)...\n",
      "Combined Training Shape: (70988, 312)\n",
      "Training on 140 proteins...\n",
      "Processed 20/140 proteins\n",
      "Processed 40/140 proteins\n",
      "Processed 60/140 proteins\n",
      "Processed 80/140 proteins\n",
      "Processed 100/140 proteins\n",
      "Processed 120/140 proteins\n",
      "Processed 140/140 proteins\n",
      "\n",
      "SUCCESS: Grand Finale complete.\n",
      "Submission saved to: /home/skovtun/Python_projects/Kaggle/Single_cell/data/Eval_cite/X_eval_results.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- Grand Finale: Full Retrain & Submission ---\n",
    "# Strategy: \n",
    "# 1. Combine Train + Test sets into one massive training matrix (Maximize Data).\n",
    "# 2. Retrain the \"Deep & Rigid\" model on this combined data.\n",
    "# 3. Predict on the Evaluation set for the final Leaderboard submission.\n",
    "print(\"Initiating Grand Finale: Retraining on ALL available data (Train + Test)...\")\n",
    "\n",
    "# 1. Concatenate Data (Maximize Signal)\n",
    "# We join the training and testing sets to give the model the most complete view of biology.\n",
    "X_total = pd.concat([X_train, X_test])\n",
    "y_total = pd.concat([y_train, y_test])\n",
    "\n",
    "print(f\"Combined Training Shape: {X_total.shape}\")\n",
    "\n",
    "# 2. Preprocessing\n",
    "# Keep 'day', drop 'donor' (as per our winning strategy).\n",
    "X_total_feat = to_float32(X_total.drop(columns=['donor']))\n",
    "X_eval_feat  = to_float32(X_eval.drop(columns=['donor']))\n",
    "\n",
    "# Create DMatrices (Optimized for speed)\n",
    "dtrain_final = xgb.DMatrix(X_total_feat)\n",
    "deval_final  = xgb.DMatrix(X_eval_feat)\n",
    "\n",
    "# 3. Output Container\n",
    "y_pred_eval = pd.DataFrame(index=X_eval.index, columns=y_train.columns, dtype=np.float32)\n",
    "\n",
    "# 4. Final Parameters (The Verified Winner)\n",
    "final_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"seed\": 77,\n",
    "    \"verbosity\": 0,\n",
    "    \n",
    "    # \"Deep & Rigid\" Config\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,              # Deep trees for complex biology\n",
    "    \"min_child_weight\": 30,      \n",
    "    \"subsample\": 0.85, \n",
    "    \"colsample_bytree\": 0.4,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 15.0,          # Strong regularization for robustness\n",
    "    \"gamma\": 0.0,\n",
    "}\n",
    "\n",
    "# 5. The Final Training Loop\n",
    "# We use 700 rounds. Since we have MORE data now (Train+Test), \n",
    "# 700 is a conservative safe zone to prevent underfitting without exploding runtime.\n",
    "NUM_ROUNDS = 700\n",
    "\n",
    "print(f\"Training on {len(y_train.columns)} proteins...\")\n",
    "\n",
    "for i, protein in enumerate(y_train.columns, 1):\n",
    "    # Set label for the combined dataset\n",
    "    y_target = y_total[protein].to_numpy(dtype=np.float32)\n",
    "    dtrain_final.set_label(y_target)\n",
    "\n",
    "    # Train\n",
    "    booster = xgb.train(\n",
    "        params=final_params,\n",
    "        dtrain=dtrain_final,\n",
    "        num_boost_round=NUM_ROUNDS,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Predict on Evaluation Set\n",
    "    y_pred_eval[protein] = booster.predict(deval_final)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processed {i}/{len(y_train.columns)} proteins\")\n",
    "\n",
    "# 6. Save Final Submission\n",
    "submission_path = os.path.join(data_dir, \"Eval_cite\", \"X_eval_results.parquet\")\n",
    "os.makedirs(os.path.dirname(submission_path), exist_ok=True)\n",
    "\n",
    "y_pred_eval.to_parquet(\n",
    "    submission_path,\n",
    "    engine=\"pyarrow\",\n",
    "    index=True,\n",
    "    coerce_timestamps=\"ms\",\n",
    "    allow_truncated_timestamps=True,\n",
    "    version=\"2.6\"\n",
    ")\n",
    "\n",
    "print(f\"\\nSUCCESS: Grand Finale complete.\")\n",
    "print(f\"Submission saved to: {submission_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937aec8c-4515-4272-9d1f-fac072e511cb",
   "metadata": {},
   "source": [
    "Kaggle Pierson correlation train(old best version): 0.9253355768254198\n",
    "Kaggle Pierson correlation train (old best version) : 0.9014217099264067\n",
    "Train Score (Pearson): 0.94234\n",
    "Test Score  (Pearson): 0.90156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e37bc-bcf2-499d-9bab-a5cad5bc26cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
