{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e3dcc49-575d-42f7-be45-cd54e10a29c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121798\n"
     ]
    }
   ],
   "source": [
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4921055-bef4-4e82-ae9f-f167816e19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import tables\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Limit threads for numerical libraries to manage CPU usage\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"7\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"7\"\n",
    "\n",
    "base_dir = \"/home/skovtun/Python_projects/Kaggle/Single_cell/\"\n",
    "data_dir = os.path.join(base_dir, \"data\")\n",
    "random_state = 77\n",
    "\n",
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43acfad6-1b11-446d-b556-88e746bd7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting external file providing the mapping of human genes to their chromosome coordinates on \n",
    "#the GRCh38 genome to use for reducing number of columns for every target.\n",
    "# RAW LINE:\n",
    "# 1\thavana\tgene\t11869\t14409\t.\t+\t.\tgene_id \"ENSG00000223972\"; gene_version \"5\"; gene_name \"DDX11L1\"; gene_source \"havana\"; gene_biotype \"transcribed_unprocessed_pseudogene\";\n",
    "\n",
    "gtf_path = \"Homo_sapiens.GRCh38.98.gtf\"\n",
    "\n",
    "genes = []\n",
    "\n",
    "with open(gtf_path) as f:\n",
    "    for line in f:\n",
    "        #skipping comments\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        #splitting the line\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        if fields[2] != \"gene\":\n",
    "            continue\n",
    "        \n",
    "        chrom = fields[0]\n",
    "        start = int(fields[3])\n",
    "        end = int(fields[4])\n",
    "        \n",
    "        attr = fields[8]\n",
    "        match_id = re.search(r'gene_id \"([^\"]+)\"',attr)\n",
    "        match_name = re.search(r'gene_name \"([^\"]+)\"',attr)\n",
    "        if match_id:\n",
    "            gene_id = match_id.group(1)\n",
    "        else:\n",
    "            continue\n",
    "        if match_name:\n",
    "            gene_name = match_name.group(1)\n",
    "        else:\n",
    "            gene_name = None\n",
    "        \n",
    "        genes.append([gene_id, gene_name, chrom, start, end])\n",
    "\n",
    "gene_df = pd.DataFrame(genes, columns=[\"gene_id\", \"gene_name\", \"chr\", \"start\", \"end\"])\n",
    "mapping = {str(i): f\"chr{i}\" for i in range(1,23)}\n",
    "mapping['X'] = 'chrX'\n",
    "mapping['Y'] = 'chrY'\n",
    "gene_df['chr'] = gene_df['chr'].map(mapping).fillna(gene_df['chr'])\n",
    "#gene_df['chr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3bcbb224-4875-400d-becf-0da414d0c8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60623, 5)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3695775a-c526-40d2-aa09-1b2396508965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23404, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting a list of genes and all coordinates, parsing ATAC Peaks\n",
    "path = \"train_multi_inputs.h5\"\n",
    "cols = pd.read_hdf(path, key=\"train_multi_inputs\", start=0, stop=1).columns\n",
    "path = \"train_multi_targets.h5\"\n",
    "cols_t = pd.read_hdf(path, key = 'train_multi_targets', start = 0, stop=1).columns\n",
    "\n",
    "#From the column names of the multi creating a dataframe with location name, start and end.\n",
    "arr = pd.Series(cols.values)\n",
    "r_p = r'([^:]+):([\\d]+)-([\\d]+)'\n",
    "chr_ranges = arr.str.extract(r_p)\n",
    "chr_ranges.columns = ['chr','start','end']\n",
    "chr_ranges['start'] = chr_ranges['start'].astype(int)\n",
    "chr_ranges['end'] = chr_ranges['end'].astype(int)\n",
    "\n",
    "#Reducing gene_df by choosing onnly chr present in chr_ranges and only gene_id's present as targets.\n",
    "multi_chr = list(chr_ranges['chr'].unique())\n",
    "gene_df_multi = gene_df[gene_df['chr'].isin(multi_chr)]\n",
    "gene_df_multi = gene_df_multi.set_index('gene_id')\n",
    "missing = cols_t.difference(gene_df_multi.index)\n",
    "gene_df_multi = gene_df_multi.loc[gene_df_multi.index.intersection(cols_t)]\n",
    "gene_df_multi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f0065f-e88b-4b24-bc4f-2bdaebd109be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating maximum amount of features for every target.\n",
    "window = 2000\n",
    "\n",
    "genes = gene_df_multi.loc[gene_df_multi.index.intersection(cols_t)].copy()\n",
    "genes[\"left\"]  = genes[\"start\"] - window\n",
    "genes[\"right\"] = genes[\"end\"]   + window\n",
    "genes[\"gene_id\"] = genes.index  # preserve gene_id as a column\n",
    "\n",
    "chr_ranges = chr_ranges.rename(columns={\n",
    "    \"start\": \"start_peak\",\n",
    "    \"end\":   \"end_peak\"\n",
    "})\n",
    "\n",
    "target_cols = pd.DataFrame()\n",
    "results = []\n",
    "gene_id = []\n",
    "start_peak = []\n",
    "end_peak = []\n",
    "\n",
    "for chr_name, chr_peaks in chr_ranges.groupby(\"chr\"):\n",
    "    sub_genes = genes[genes[\"chr\"] == chr_name]\n",
    "    if sub_genes.empty or chr_peaks.empty:\n",
    "        continue\n",
    "\n",
    "    merged = sub_genes[['gene_id', 'chr', 'left', 'right']].merge(chr_peaks[['chr', 'start_peak', 'end_peak']], on=\"chr\")\n",
    "\n",
    "    mask = (\n",
    "        (merged[\"start_peak\"] >= merged[\"left\"]) &\n",
    "        (merged[\"end_peak\"]   <= merged[\"right\"])\n",
    "    )\n",
    "    m = merged[mask]\n",
    "    results.append(m)\n",
    "        \n",
    "target_cols = pd.concat(results)\n",
    "target_cols['col'] = target_cols['chr'].astype(str)+\":\"+target_cols['start_peak'].astype(str)+\"-\"+target_cols['end_peak'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846905f1-c2dc-45df-8db1-517c5168efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed features from Notebook 1...\n",
      "âœ… Data Loaded Successfully.\n",
      "X (Features): (105942, 1000) - 1000 Dimensions\n",
      "Y (Targets):  (105942, 300) - 300 Dimensions\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pre-computed features from Notebook 1...\")\n",
    "\n",
    "# 1. Load the SVD-compressed Chromatin Features (Input)\n",
    "# Shape: (105942, 1000)\n",
    "X_csr_1000 = np.load(\"X_csr_1000.npy\")\n",
    "\n",
    "# 2. Load the PCA-compressed RNA Targets (Output)\n",
    "# Shape: (105942, 300)\n",
    "n_cells = X_csr_1000.shape[0]\n",
    "Y_pca = np.memmap(\"Y_train_pca_300.f32\", dtype=\"float32\", mode=\"r\", shape=(n_cells, 300))\n",
    "\n",
    "# 3. Load PCA Components (Decoder)\n",
    "# Shape: (300, 23418)\n",
    "Y_components = np.load(\"Y_ipca_components_300.npy\")\n",
    "Y_mean = np.load(\"Y_ipca_mean.npy\")\n",
    "\n",
    "print(f\"âœ… Data Loaded Successfully.\")\n",
    "print(f\"X (Features): {X_csr_1000.shape} - 1000 Dimensions\")\n",
    "print(f\"Y (Targets):  {Y_pca.shape} - 300 Dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89349dec-6cf5-49d0-b82f-e374c0be3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_importance = np.sum(np.abs(Y_components), axis = 0)\n",
    "ranked_genes = pd.Series(gene_importance, index = cols_t).sort_values(ascending= False)\n",
    "genes_1000 = ranked_genes[:1000]\n",
    "target_cols_1000 = target_cols[target_cols['gene_id'].isin(genes_1000.index)]\n",
    "peaks_1000 = target_cols_1000['col'].unique()\n",
    "with tables.open_file(\"train_multi_inputs.h5\", \"r\") as f:\n",
    "    peaks = f.get_node(\"/train_multi_inputs/axis0\")[:]\n",
    "peaks_d = np.char.decode(peaks, encoding = 'utf-8')\n",
    "peak_to_id = {str(peak): i for i,peak in enumerate(peaks_d)}\n",
    "peaks_1000_idx = sorted(set([peak_to_id[peak] for peak in peaks_1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23d61fa-fa0e-4c2e-a0bc-756c1e7be999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (105942, 228942)\n",
      "Sliced shape: (105942, 15581)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "X_crs = scipy.sparse.load_npz('train_multi_cell.npz')\n",
    "Xgene_1000 = X_crs[:,peaks_1000_idx]\n",
    "print(f\"Original shape: {X_crs.shape}\")\n",
    "print(f\"Sliced shape: {Xgene_1000.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2bf365b-b620-4aec-bd42-35fee01264d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105942, 16581)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "X_pca_sparse = csr_matrix(X_csr_1000)\n",
    "X = hstack([X_pca_sparse,Xgene_1000])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7f0b4d-83c4-4b92-baa9-7e1d43d1b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting and Feature Separation ---\n",
    "# Random Train-Test Split (80/20)\n",
    "with tables.open_file(\"train_multi_inputs.h5\", \"r\") as f:\n",
    "    values = f.get_node(\"/train_multi_inputs/axis1\")[:]\n",
    "cell_names = np.char.decode(values, encoding = 'utf-8')\n",
    "cell_to_id = {str(name): i for i,name in enumerate(cell_names)}\n",
    "\n",
    "\n",
    "random_state=77\n",
    "rng = np.random.default_rng(random_state)\n",
    "shuffled_names = rng.permutation(cell_names)\n",
    "\n",
    "split_point = int(len(shuffled_names) * 0.8)\n",
    "train_names = shuffled_names[:split_point]\n",
    "test_names = shuffled_names[split_point:]\n",
    "\n",
    "train_id = sorted([cell_to_id[name] for name in train_names])\n",
    "test_id = sorted([cell_to_id[name] for name in test_names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e5ed97e9-bca0-406a-8b2c-687b070adb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H5 Names Count: 105942\n",
      "Meta Rows Count: 105942\n",
      "âœ… VERIFIED: The metadata is perfectly aligned with the H5 file.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Length Check: Do we have the same number of rows?\n",
    "print(f\"H5 Names Count: {len(cell_names)}\")\n",
    "print(f\"Meta Rows Count: {len(multi_metadata)}\")\n",
    "assert len(cell_names) == len(multi_metadata), \"âŒ Mismatch in length!\"\n",
    "\n",
    "# 2. Order Check: Is every single name in the exact same spot?\n",
    "# We compare the list of names from H5 vs the Index of the Dataframe\n",
    "are_aligned = np.array_equal(multi_metadata.index.values, cell_names)\n",
    "\n",
    "if are_aligned:\n",
    "    print(\"âœ… VERIFIED: The metadata is perfectly aligned with the H5 file.\")\n",
    "else:\n",
    "    print(\"âŒ WARNING: The order is different!\")\n",
    "    # Show the first mismatch if it failed\n",
    "    for i, (name_h5, name_meta) in enumerate(zip(cell_names, multi_metadata.index)):\n",
    "        if name_h5 != name_meta:\n",
    "            print(f\"First mismatch at row {i}:\")\n",
    "            print(f\"  H5 says:   {name_h5}\")\n",
    "            print(f\"  Meta says: {name_meta}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f3d373d-d6ad-4a54-8954-6ef5a8d31116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84753, 16581) (21189, 16581) (84753, 100) (21189, 100)\n"
     ]
    }
   ],
   "source": [
    "inputs_train, inputs_test = X[train_id,:], X[test_id,:]\n",
    "y_subset = Y_pca[:,:100]\n",
    "y_train, y_test = y_subset[train_id,:], y_subset[test_id,:]\n",
    "print(inputs_train.shape, inputs_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82df1dc3-0677-46f6-993e-618c98b08ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading metatdata\n",
    "metadata_old = pd.read_csv('metadata.csv', index_col = 'cell_id')\n",
    "fix = pd.read_csv('metadata_cite_day_2_donor_27678.csv', index_col = 'cell_id')\n",
    "metadata = pd.concat([metadata_old, fix], axis = 0)\n",
    "del metadata_old, fix\n",
    "multi_metadata  = metadata.loc[cell_names,['day','donor']]\n",
    "del metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92f0f77a-4e4f-4720-bec0-6bd13b2e5915",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train, meta_test = multi_metadata.iloc[train_id], multi_metadata.iloc[test_id]\n",
    "day_col_train = meta_train['day'].values.reshape(-1,1)\n",
    "day_sparse_train = csr_matrix(day_col_train)\n",
    "x_train = hstack([inputs_train,day_sparse_train])\n",
    "\n",
    "day_col_test = meta_test['day'].values.reshape(-1,1)\n",
    "day_sparse_test = csr_matrix(day_col_test)\n",
    "x_test = hstack([inputs_test, day_sparse_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86802df6-8daf-4377-8d22-dd3ffc8540cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84753, 16582)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d4beae5-82ec-4325-b3bb-21ea45844d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Randomly Selected Holdout Donor: 32606\n"
     ]
    }
   ],
   "source": [
    "# 1. Choosing a donor to_hold_out\n",
    "donors = meta_train['donor'].unique()\n",
    "rng = np.random.default_rng(seed=77) \n",
    "val_donor = rng.choice(donors)\n",
    "\n",
    "print(f\" Randomly Selected Holdout Donor: {val_donor}\")\n",
    "\n",
    "# 2. Create Indices based on that random choice\n",
    "tr_idx = np.where(meta_train['donor']!= val_donor)[0]\n",
    "tr_mask = (meta_train['donor'] != val_donor).values\n",
    "\n",
    "va_idx = np.where(meta_train['donor'] == val_donor)[0]\n",
    "va_mask = (meta_train['donor'] == val_donor).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eeb6e7f6-c724-4fc2-acc8-dbd68eb7cfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tr_mask[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a72a088-7dac-425a-a717-b34460bd24a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Tuning Candidates B & C (PC1 Only)...\n",
      "   (Comparing against expected baseline ~23.0)\n",
      "Train size: 57824 | Validation size: 26929\n",
      "\n",
      "--- Testing Candidate 2 ---\n",
      "   >> PC1 RMSE: 23.71737\n",
      "\n",
      "--- Testing Candidate 3 ---\n",
      "   >> PC1 RMSE: 24.18898\n",
      "\n",
      "--- ðŸ Tuning Complete ---\n",
      "Winner of Round 2:\n",
      "   RMSE: 23.71737\n",
      "   Params: {'max_depth': 10, 'learning_rate': 0.03, 'min_child_weight': 50, 'colsample_bytree': 0.4, 'subsample': 0.7, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import gc \n",
    "\n",
    "# --- 1. Define Candidates B and C ---\n",
    "candidate_params_remaining = [\n",
    "    # Candidate B: \"The Deep Miner\" \n",
    "    # (Depth 10, Slow but powerful)\n",
    "    dict(\n",
    "        max_depth=10, \n",
    "        learning_rate=0.03,       \n",
    "        min_child_weight=50,      \n",
    "        colsample_bytree=0.4, \n",
    "        subsample=0.7, \n",
    "        reg_alpha=0.5, \n",
    "        reg_lambda=5.0            \n",
    "    ),\n",
    "\n",
    "    # Candidate C: \"The Feature Scanner\" \n",
    "    # (Depth 7, Fast and diverse)\n",
    "    dict(\n",
    "        max_depth=7, \n",
    "        learning_rate=0.05,\n",
    "        min_child_weight=30, \n",
    "        colsample_bytree=0.15,    \n",
    "        subsample=0.9, \n",
    "        reg_alpha=0.1, \n",
    "        reg_lambda=2.0\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"ðŸš€ Tuning Candidates B & C (PC1 Only)...\")\n",
    "print(f\"   (Comparing against expected baseline ~23.0)\")\n",
    "# Slice the Sparse Matrix (x_train) and Dense Targets (Y_dev)\n",
    "# CRITICAL: We only use the Top 5 PCA components for tuning speed\n",
    "X_tr = x_train[tr_mask]\n",
    "X_va = x_train[va_mask]\n",
    "\n",
    "Y_tr = y_train[tr_mask][:, :1]  # Top 1 components only\n",
    "Y_va = y_train[va_mask][:, :1]\n",
    "\n",
    "print(f\"Train size: {X_tr.shape[0]} | Validation size: {X_va.shape[0]}\")\n",
    "scores = []\n",
    "\n",
    "# Start enumeration at 2 (Candidate A was 1)\n",
    "for k, cand in enumerate(candidate_params_remaining, 2):\n",
    "    print(f\"\\n--- Testing Candidate {k} ---\")\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"verbosity\": 0,\n",
    "        \"n_jobs\": -1,\n",
    "        **cand\n",
    "    }\n",
    "\n",
    "    # NO LOOPS. Just Index 0 (PC1).\n",
    "    j = 0 \n",
    "    \n",
    "    # 1. Create DMatrices (PC1 Only)\n",
    "    dtrain = xgb.DMatrix(X_tr, label=Y_tr[:, j])\n",
    "    dvalid = xgb.DMatrix(X_va, label=Y_va[:, j])\n",
    "    \n",
    "    # 2. Train\n",
    "    booster = xgb.train(\n",
    "        params, \n",
    "        dtrain, \n",
    "        num_boost_round=1000,\n",
    "        evals=[(dvalid, \"val\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # 3. Capture Score\n",
    "    best_rmse = booster.best_score\n",
    "    scores.append({\"cand\": cand, \"score\": best_rmse})\n",
    "    \n",
    "    print(f\"   >> PC1 RMSE: {best_rmse:.5f}\")\n",
    "\n",
    "    # 4. Clean Memory immediately\n",
    "    del dtrain, dvalid, booster\n",
    "    gc.collect()\n",
    "\n",
    "# --- Final Decision ---\n",
    "print(\"\\n--- ðŸ Tuning Complete ---\")\n",
    "# Compare B and C\n",
    "winner = min(scores, key=lambda x: x[\"score\"])\n",
    "\n",
    "print(f\"Winner of Round 2:\")\n",
    "print(f\"   RMSE: {winner['score']:.5f}\")\n",
    "print(f\"   Params: {winner['cand']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323d575-34de-4b37-9dda-61775d9ced03",
   "metadata": {},
   "outputs": [],
   "source": [
    " Randomly Selected Holdout Donor: 32606\n",
    "Hyperparameter Tuning on Holdout Donor: 32606\n",
    "Train size: 57824 | Validation size: 26929\n",
    "\n",
    "Starting tuning on Top 5 PCA components...\n",
    "[1/3] Avg RMSE: 11.31940 | Params: {'max_depth': 6, 'learning_rate': 0.05, 'min_child_weight': 20, 'colsample_bytree': 0.3, 'subsample': 0.8, 'reg_alpha': 1.0, 'reg_lambda': 1.0}\n",
    "\n",
    "Randomly Selected Holdout Donor: 32606\n",
    "Hyperparameter Tuning on Holdout Donor: 32606\n",
    "Train size: 57824 | Validation size: 26929\n",
    "\n",
    "Starting tuning on Top 5 PCA components...\n",
    "[1/3] Avg RMSE: 23.85963 | Params: {'max_depth': 6, 'learning_rate': 0.05, 'min_child_weight': 20, 'colsample_bytree': 0.3, 'subsample': 0.8, 'reg_alpha': 1.0, 'reg_lambda': 1.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7b44517-a60a-4bce-99ca-db90aa80e716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4103"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dtrain, dvalid, X_tr, X_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e21294bd-ba11-4644-afe9-72dcb9b25341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (57824, 16582) | Val: (26929, 16582) | PCs in panel: 4 -> [1, 4, 9, 29]\n",
      "Using QuantileDMatrix\n",
      "\n",
      "=== Evaluating Candidate A on PCs [1, 4, 9, 29] ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 140\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nm, cand \u001b[38;5;129;01min\u001b[39;00m CANDS:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Evaluating Candidate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on PCs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpc_panel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     res = \u001b[43meval_candidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     results.append(res)\n\u001b[32m    142\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCandidate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: rmse_mean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33mrmse_mean\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | nrmse_mean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33mnrmse_mean\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    143\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m| best_iter_mean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33mbest_iter_mean\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | time=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33mtime_s\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36meval_candidate\u001b[39m\u001b[34m(name, cand_params, num_boost_round, early_stop)\u001b[39m\n\u001b[32m     97\u001b[39m dtrain.set_float_info(\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m, ytr)\n\u001b[32m     98\u001b[39m dvalid.set_float_info(\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m, yva)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m booster = \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m rmse = \u001b[38;5;28mfloat\u001b[39m(booster.best_score)\n\u001b[32m    110\u001b[39m best_iter = \u001b[38;5;28mint\u001b[39m(booster.best_iteration)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/xgboost/core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/xgboost/training.py:200\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    199\u001b[39m     bst.update(dtrain, iteration=i, fobj=obj)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    203\u001b[39m bst = cb_container.after_training(bst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/xgboost/callback.py:266\u001b[39m, in \u001b[36mCallbackContainer.after_iteration\u001b[39m\u001b[34m(self, model, epoch, dtrain, evals)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m name.find(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m) == -\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDataset name should not contain `-`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m score: \u001b[38;5;28mstr\u001b[39m = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_margin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m metric_score = _parse_eval_str(score)\n\u001b[32m    268\u001b[39m \u001b[38;5;28mself\u001b[39m._update_history(metric_score, epoch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/xgboost/core.py:2540\u001b[39m, in \u001b[36mBooster.eval_set\u001b[39m\u001b[34m(self, evals, iteration, feval, output_margin)\u001b[39m\n\u001b[32m   2537\u001b[39m evnames = c_array(ctypes.c_char_p, [c_str(d[\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evals])\n\u001b[32m   2538\u001b[39m msg = ctypes.c_char_p()\n\u001b[32m   2539\u001b[39m _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2540\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterEvalOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc_bst_ulong\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2548\u001b[39m )\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m msg.value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2550\u001b[39m res = msg.value.decode()  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Efficient A vs B comparison\n",
    "# - Reuse QuantileDMatrix/DMatrix (build once)\n",
    "# - Reuse feature binning (hist)\n",
    "# - Update labels in-place per PC (no DMatrix rebuild)\n",
    "# - Evaluate on a small PC panel and aggregate RMSE + nRMSE\n",
    "# -----------------------------\n",
    "\n",
    "# ====== Choose the PC panel (edit if you want) ======\n",
    "PC_PANEL = [1, 4, 9, 29]  # PCs: 1,2,5,10,30,60,90 (0-based)\n",
    "# If you only have 100 PCs, index 89 is fine. If fewer, filter automatically below.\n",
    "\n",
    "# ====== Two candidates to compare ======\n",
    "cand_A = dict(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=20,\n",
    "    colsample_bytree=0.3,\n",
    "    subsample=0.8,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=1.0\n",
    ")\n",
    "\n",
    "cand_B_prime = dict(\n",
    "    max_depth=8,        # â†“ from 10 (huge win)\n",
    "    learning_rate=0.04, # â†‘ slightly\n",
    "    min_child_weight=50,\n",
    "    colsample_bytree=0.35,\n",
    "    subsample=0.7,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=5.0\n",
    ")\n",
    "\n",
    "CANDS = [(\"A\", cand_A), (\"B\", cand_B)]\n",
    "\n",
    "# ====== Base params tuned for laptop efficiency ======\n",
    "BASE_PARAMS = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    num_boost_round=1200,\n",
    "    tree_method=\"hist\",\n",
    "    max_bin=256,      # speed/CPU-friendly\n",
    "    n_jobs=8,         # avoid -1 on laptop (thermal/OS contention)\n",
    "    verbosity=0,\n",
    "    seed=77,\n",
    ")\n",
    "\n",
    "# ====== Slice once (avoid repeated CSR fancy indexing cost) ======\n",
    "X_tr = x_train[tr_mask]\n",
    "X_va = x_train[va_mask]\n",
    "\n",
    "Y_tr_full = y_train[tr_mask]\n",
    "Y_va_full = y_train[va_mask]\n",
    "\n",
    "n_targets = Y_tr_full.shape[1]\n",
    "pc_panel = [j for j in PC_PANEL if j < n_targets]\n",
    "if len(pc_panel) == 0:\n",
    "    raise ValueError(f\"PC_PANEL indices {PC_PANEL} are out of range for n_targets={n_targets}\")\n",
    "\n",
    "# Make sure float32 (faster + smaller)\n",
    "Y_tr_full = np.asarray(Y_tr_full, dtype=np.float32, order=\"C\")\n",
    "Y_va_full = np.asarray(Y_va_full, dtype=np.float32, order=\"C\")\n",
    "\n",
    "print(f\"Train: {X_tr.shape} | Val: {X_va.shape} | PCs in panel: {len(pc_panel)} -> {pc_panel}\")\n",
    "\n",
    "# ====== Build matrix objects ONCE ======\n",
    "# QuantileDMatrix usually faster + more memory efficient for hist.\n",
    "# If it fails (older xgboost), fall back to DMatrix.\n",
    "use_qdm = True\n",
    "try:\n",
    "    dtrain = xgb.QuantileDMatrix(X_tr, label=Y_tr_full[:, pc_panel[0]], max_bin=BASE_PARAMS[\"max_bin\"])\n",
    "    dvalid = xgb.QuantileDMatrix(X_va, label=Y_va_full[:, pc_panel[0]], ref=dtrain, max_bin=BASE_PARAMS[\"max_bin\"])\n",
    "    print(\"Using QuantileDMatrix\")\n",
    "except Exception:\n",
    "    use_qdm = False\n",
    "    dtrain = xgb.DMatrix(X_tr, label=Y_tr_full[:, pc_panel[0]])\n",
    "    dvalid = xgb.DMatrix(X_va, label=Y_va_full[:, pc_panel[0]])\n",
    "    print(\"Using DMatrix\")\n",
    "\n",
    "# ====== Helper: run one candidate across PC panel with in-place label swap ======\n",
    "def eval_candidate(name, cand_params, num_boost_round=2000, early_stop=50):\n",
    "    params = {**BASE_PARAMS, **cand_params}\n",
    "\n",
    "    per_pc = []\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    for j in pc_panel:\n",
    "        ytr = Y_tr_full[:, j]\n",
    "        yva = Y_va_full[:, j]\n",
    "\n",
    "        # Update labels in-place (fast; avoids rebuilding dtrain/dvalid)\n",
    "        dtrain.set_float_info(\"label\", ytr)\n",
    "        dvalid.set_float_info(\"label\", yva)\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            evals=[(dvalid, \"val\")],\n",
    "            early_stopping_rounds=early_stop,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        rmse = float(booster.best_score)\n",
    "        best_iter = int(booster.best_iteration)\n",
    "\n",
    "        # Normalized RMSE (robust if PCs not whitened)\n",
    "        ystd = float(np.std(yva))\n",
    "        nrmse = rmse / ystd if ystd > 0 else np.nan\n",
    "\n",
    "        per_pc.append((j, rmse, nrmse, best_iter))\n",
    "\n",
    "        del booster\n",
    "        gc.collect()\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    rmse_mean = float(np.mean([x[1] for x in per_pc]))\n",
    "    nrmse_mean = float(np.nanmean([x[2] for x in per_pc]))\n",
    "    it_mean = float(np.mean([x[3] for x in per_pc]))\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"time_s\": t1 - t0,\n",
    "        \"rmse_mean\": rmse_mean,\n",
    "        \"nrmse_mean\": nrmse_mean,\n",
    "        \"best_iter_mean\": it_mean,\n",
    "        \"per_pc\": per_pc\n",
    "    }\n",
    "\n",
    "# ====== Run A vs B ======\n",
    "results = []\n",
    "for nm, cand in CANDS:\n",
    "    print(f\"\\n=== Evaluating Candidate {nm} on PCs {pc_panel} ===\")\n",
    "    res = eval_candidate(nm, cand)\n",
    "    results.append(res)\n",
    "    print(f\"Candidate {nm}: rmse_mean={res['rmse_mean']:.5f} | nrmse_mean={res['nrmse_mean']:.5f} \"\n",
    "          f\"| best_iter_mean={res['best_iter_mean']:.1f} | time={res['time_s']:.1f}s\")\n",
    "\n",
    "# ====== Decide winner (prefer nRMSE for cross-PC comparability) ======\n",
    "winner_by_nrmse = min(results, key=lambda r: r[\"nrmse_mean\"])\n",
    "winner_by_rmse  = min(results, key=lambda r: r[\"rmse_mean\"])\n",
    "\n",
    "print(\"\\n--- Summary (per-PC) ---\")\n",
    "for r in results:\n",
    "    print(f\"\\nCandidate {r['name']} details:\")\n",
    "    for (j, rmse, nrmse, it) in r[\"per_pc\"]:\n",
    "        print(f\"  PC{j+1:>3}: RMSE={rmse:>8.4f} | nRMSE={nrmse:>8.4f} | best_iter={it}\")\n",
    "\n",
    "print(\"\\n--- Winners ---\")\n",
    "print(f\"Winner by mean nRMSE (recommended): Candidate {winner_by_nrmse['name']}\")\n",
    "print(f\"Winner by mean RMSE:              Candidate {winner_by_rmse['name']}\")\n",
    "\n",
    "# Cleanup big objects\n",
    "del dtrain, dvalid, X_tr, X_va\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5055bc5-6f61-4b18-b9fa-ff6dd0e8a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fast A-vs-B tuning for Multiome PCA targets on a laptop\n",
    "# - Converts CSR to float32 (+ int32 indices)\n",
    "# - Row-subsamples train/val for tuning\n",
    "# - Uses DMatrix (reliable label swapping)\n",
    "# - Replaces \"depth=10\" with lossguide + max_leaves (much faster)\n",
    "# - Selects by Pearson over CELLS (per PC), not RMSE\n",
    "# - Includes a timing \"pilot\" block to estimate full runtime\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# -----------------------------\n",
    "# USER SETTINGS\n",
    "# -----------------------------\n",
    "PC_PANEL = [1, 4, 9, 29]          # 0-based PCs -> [PC2, PC5, PC10, PC30]\n",
    "TRAIN_SUB = 20000                 # tuning subsample sizes\n",
    "VAL_SUB   = 12000\n",
    "\n",
    "NUM_BOOST_ROUND = 800             # cap work\n",
    "EARLY_STOP = 30\n",
    "VERBOSE_EVAL = False              # set to 50 if you want to see progress\n",
    "\n",
    "N_JOBS = 8\n",
    "MAX_BIN = 128                     # 128 for tuning speed; use 256 later if needed\n",
    "\n",
    "SEED = 77\n",
    "\n",
    "# -----------------------------\n",
    "# CANDIDATES (A + B_fast)\n",
    "# -----------------------------\n",
    "cand_A = dict(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=20,\n",
    "    colsample_bytree=0.30,\n",
    "    subsample=0.80,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=1.0,\n",
    ")\n",
    "\n",
    "# Replace depth-10 \"B\" with a compute-controlled variant.\n",
    "# lossguide grows best-first with a leaf budget; typically much faster.\n",
    "cand_B_fast = dict(\n",
    "    grow_policy=\"lossguide\",\n",
    "    max_leaves=256,          # try 128 if still slow, or 512 if you want more capacity\n",
    "    max_depth=0,             # ignored for lossguide; keep 0\n",
    "    learning_rate=0.05,      # slightly higher than 0.03 for fewer rounds\n",
    "    min_child_weight=50,\n",
    "    colsample_bytree=0.35,\n",
    "    subsample=0.70,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=5.0,\n",
    ")\n",
    "\n",
    "CANDS = [(\"A\", cand_A), (\"B_fast\", cand_B_fast)]\n",
    "\n",
    "BASE_PARAMS = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",      # used for early stopping only\n",
    "    tree_method=\"hist\",\n",
    "    max_bin=MAX_BIN,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbosity=0,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# SAFETY CHECKS / PREP\n",
    "# -----------------------------\n",
    "assert sp.isspmatrix_csr(x_train), \"Expected x_train to be CSR matrix.\"\n",
    "assert y_train.ndim == 2, \"Expected y_train shape (n_samples, n_pcs).\"\n",
    "\n",
    "# Filter PC panel to available targets\n",
    "n_targets = y_train.shape[1]\n",
    "pc_panel = [j for j in PC_PANEL if 0 <= j < n_targets]\n",
    "if not pc_panel:\n",
    "    raise ValueError(f\"PC_PANEL {PC_PANEL} out of range. n_targets={n_targets}\")\n",
    "\n",
    "print(f\"PC panel (0-based): {pc_panel} (n={len(pc_panel)})\")\n",
    "print(f\"Train/Val full shapes: {x_train[tr_mask].shape} / {x_train[va_mask].shape}\")\n",
    "\n",
    "# Convert to float32 CSR + int32 indices (big speed win)\n",
    "# copy=False keeps memory low if already in desired dtype\n",
    "x_train32 = x_train.astype(np.float32, copy=False)\n",
    "x_train32.indices = x_train32.indices.astype(np.int32, copy=False)\n",
    "x_train32.indptr  = x_train32.indptr.astype(np.int32, copy=False)\n",
    "\n",
    "# Ensure y is float32 contiguous for fast slicing\n",
    "y_train32 = np.asarray(y_train, dtype=np.float32, order=\"C\")\n",
    "\n",
    "# -----------------------------\n",
    "# Subsample rows for tuning (HUGE speed win)\n",
    "# -----------------------------\n",
    "rng = np.random.default_rng(SEED)\n",
    "tr_idx = np.where(tr_mask)[0]\n",
    "va_idx = np.where(va_mask)[0]\n",
    "\n",
    "tr_sub = rng.choice(tr_idx, size=min(TRAIN_SUB, tr_idx.size), replace=False)\n",
    "va_sub = rng.choice(va_idx, size=min(VAL_SUB,   va_idx.size), replace=False)\n",
    "\n",
    "X_tr = x_train32[tr_sub]\n",
    "X_va = x_train32[va_sub]\n",
    "Y_tr_full = y_train32[tr_sub]\n",
    "Y_va_full = y_train32[va_sub]\n",
    "\n",
    "print(f\"Subsampled shapes: Train {X_tr.shape} | Val {X_va.shape}\")\n",
    "print(f\"X_tr nnz={X_tr.nnz:,} | density={X_tr.nnz/(X_tr.shape[0]*X_tr.shape[1]):.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def pearson_corr(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Pearson correlation between two 1D arrays. Returns nan if degenerate.\"\"\"\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    b = np.asarray(b, dtype=np.float32)\n",
    "    a = a - a.mean()\n",
    "    b = b - b.mean()\n",
    "    denom = float(np.sqrt((a*a).sum()) * np.sqrt((b*b).sum()))\n",
    "    if denom == 0.0:\n",
    "        return np.nan\n",
    "    return float((a*b).sum() / denom)\n",
    "\n",
    "def build_dmatrices(X_tr, X_va, ytr0, yva0):\n",
    "    \"\"\"Build once; later we'll swap labels via set_float_info.\"\"\"\n",
    "    dtrain = xgb.DMatrix(X_tr, label=ytr0)\n",
    "    dvalid = xgb.DMatrix(X_va, label=yva0)\n",
    "    return dtrain, dvalid\n",
    "\n",
    "def train_one_pc(dtrain, dvalid, params, ytr, yva,\n",
    "                 num_boost_round=NUM_BOOST_ROUND,\n",
    "                 early_stop=EARLY_STOP,\n",
    "                 verbose_eval=VERBOSE_EVAL):\n",
    "    \"\"\"Train for one PC; returns rmse, pearson, best_iter, elapsed_s.\"\"\"\n",
    "    dtrain.set_float_info(\"label\", ytr)\n",
    "    dvalid.set_float_info(\"label\", yva)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    booster = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(dvalid, \"val\")],\n",
    "        early_stopping_rounds=early_stop,\n",
    "        verbose_eval=verbose_eval,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    # Predict and compute Pearson over CELLS (stable!)\n",
    "    pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration + 1))\n",
    "    rmse = float(booster.best_score)\n",
    "    pr = pearson_corr(yva, pred)\n",
    "    best_iter = int(booster.best_iteration)\n",
    "\n",
    "    del booster, pred\n",
    "    gc.collect()\n",
    "    return rmse, pr, best_iter, elapsed\n",
    "\n",
    "# ============================================================\n",
    "# PILOT TIMING BLOCK (reliable estimate before full run)\n",
    "# ============================================================\n",
    "print(\"\\n=== PILOT TIMING (estimate runtime before full tuning) ===\")\n",
    "\n",
    "# Use a single mid PC (PC10 if available, else first in panel)\n",
    "pilot_pc = 9 if 9 in pc_panel else pc_panel[0]\n",
    "ytr0 = Y_tr_full[:, pilot_pc]\n",
    "yva0 = Y_va_full[:, pilot_pc]\n",
    "\n",
    "dtrain, dvalid = build_dmatrices(X_tr, X_va, ytr0, yva0)\n",
    "\n",
    "pilot_times = {}\n",
    "for name, cand in CANDS:\n",
    "    params = {**BASE_PARAMS, **cand}\n",
    "    rmse, pr, best_iter, sec = train_one_pc(dtrain, dvalid, params, ytr0, yva0)\n",
    "    pilot_times[name] = sec\n",
    "    print(f\"Pilot {name} on PC{pilot_pc+1}: time={sec:.1f}s | best_iter={best_iter} | rmse={rmse:.4f} | pearson={pr:.4f}\")\n",
    "\n",
    "# Extrapolate to full panel (rough but usually within ~20â€“30%)\n",
    "panel_factor = len(pc_panel)\n",
    "est_total_s = sum(pilot_times.values()) * panel_factor\n",
    "print(f\"\\nEstimated total time for A vs B_fast on {len(pc_panel)} PCs ~ {est_total_s/60:.1f} minutes\")\n",
    "print(\"Rule: if this estimate is too high, reduce TRAIN_SUB/VAL_SUB, max_leaves, or NUM_BOOST_ROUND.\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# FULL RUN (A vs B_fast) ON PC PANEL\n",
    "# ============================================================\n",
    "print(\"=== FULL EVALUATION (A vs B_fast) ===\")\n",
    "\n",
    "# Initialize DMatrices once with any PC labels; we'll swap inside loop\n",
    "init_pc = pc_panel[0]\n",
    "dtrain.set_float_info(\"label\", Y_tr_full[:, init_pc])\n",
    "dvalid.set_float_info(\"label\", Y_va_full[:, init_pc])\n",
    "\n",
    "results = []\n",
    "for name, cand in CANDS:\n",
    "    params = {**BASE_PARAMS, **cand}\n",
    "\n",
    "    per_pc = []\n",
    "    t_start = time.perf_counter()\n",
    "    print(f\"\\n=== Evaluating Candidate {name} on PCs {[j for j in pc_panel]} ===\")\n",
    "\n",
    "    for j in pc_panel:\n",
    "        ytr = Y_tr_full[:, j]\n",
    "        yva = Y_va_full[:, j]\n",
    "        rmse, pr, best_iter, sec = train_one_pc(dtrain, dvalid, params, ytr, yva)\n",
    "        per_pc.append((j, rmse, pr, best_iter, sec))\n",
    "        print(f\"  PC{j+1:>3}: time={sec:>6.1f}s | best_iter={best_iter:>4} | rmse={rmse:>7.3f} | pearson={pr:>7.4f}\")\n",
    "\n",
    "    total_s = time.perf_counter() - t_start\n",
    "    pearson_mean = float(np.nanmean([x[2] for x in per_pc]))\n",
    "    rmse_mean = float(np.mean([x[1] for x in per_pc]))\n",
    "\n",
    "    results.append(dict(\n",
    "        name=name,\n",
    "        pearson_mean=pearson_mean,\n",
    "        rmse_mean=rmse_mean,\n",
    "        total_s=total_s,\n",
    "        per_pc=per_pc,\n",
    "        params=cand,\n",
    "    ))\n",
    "\n",
    "    print(f\"\\nCandidate {name} summary: mean_pearson={pearson_mean:.5f} | mean_rmse={rmse_mean:.5f} | total={total_s/60:.1f} min\")\n",
    "\n",
    "# Winner by mean Pearson (recommended)\n",
    "winner = max(results, key=lambda r: r[\"pearson_mean\"])\n",
    "print(\"\\n=== WINNER (by mean Pearson over cells, averaged across PCs) ===\")\n",
    "print(f\"Winner: {winner['name']}\")\n",
    "print(f\"mean_pearson={winner['pearson_mean']:.5f} | mean_rmse={winner['rmse_mean']:.5f} | time={winner['total_s']/60:.1f} min\")\n",
    "print(\"Params:\", winner[\"params\"])\n",
    "\n",
    "# Cleanup\n",
    "del dtrain, dvalid\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "830457f8-c6a4-48c2-b6cd-8edc3ce7cc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PC2 calibration\n",
      "  Pearson:  0.88763  ->  0.88763\n",
      "  RMSE:     13.80343  ->  12.13494\n",
      "  std_ratio(pred/true): 0.928\n",
      "  bias(pred-true mean): 6.49391\n",
      "  affine: a=0.95695, b=-6.32698\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# ---- choose which PC to diagnose (0-based) ----\n",
    "j = 1   # PC2\n",
    "\n",
    "# ---- ensure float32 CSR + int32 indices (fast) ----\n",
    "x_train32 = x_train.astype(np.float32, copy=False)\n",
    "x_train32.indices = x_train32.indices.astype(np.int32, copy=False)\n",
    "x_train32.indptr  = x_train32.indptr.astype(np.int32, copy=False)\n",
    "\n",
    "y_train32 = np.asarray(y_train, dtype=np.float32, order=\"C\")\n",
    "\n",
    "# ---- slice your existing val split ----\n",
    "X_tr = x_train32[tr_mask]\n",
    "X_va = x_train32[va_mask]\n",
    "y_tr = y_train32[tr_mask, j]\n",
    "y_va = y_train32[va_mask, j]\n",
    "\n",
    "# ---- Candidate A params (as before) ----\n",
    "params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    tree_method=\"hist\",\n",
    "    max_bin=128,\n",
    "    n_jobs=8,\n",
    "    verbosity=0,\n",
    "    seed=77,\n",
    "\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=20,\n",
    "    colsample_bytree=0.30,\n",
    "    subsample=0.80,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=1.0,\n",
    ")\n",
    "\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "dvalid = xgb.DMatrix(X_va, label=y_va)\n",
    "\n",
    "booster = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=800,\n",
    "    evals=[(dvalid, \"val\")],\n",
    "    early_stopping_rounds=30,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration + 1)).astype(np.float32, copy=False)\n",
    "\n",
    "# ---- calibration functions (drop-in) ----\n",
    "def pearson(a, b):\n",
    "    a = np.asarray(a, dtype=np.float32); b = np.asarray(b, dtype=np.float32)\n",
    "    a = a - a.mean(); b = b - b.mean()\n",
    "    denom = np.sqrt((a*a).sum()) * np.sqrt((b*b).sum())\n",
    "    return np.nan if denom == 0 else float((a*b).sum() / denom)\n",
    "\n",
    "def rmse(y, p):\n",
    "    y = np.asarray(y, dtype=np.float32); p = np.asarray(p, dtype=np.float32)\n",
    "    return float(np.sqrt(np.mean((y - p) ** 2)))\n",
    "\n",
    "def fit_affine(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=np.float32)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float32)\n",
    "    xm = float(y_pred.mean()); ym = float(y_true.mean())\n",
    "    xvar = float(((y_pred - xm) ** 2).mean())\n",
    "    if xvar == 0.0:\n",
    "        a, b = 0.0, ym\n",
    "    else:\n",
    "        cov = float(((y_pred - xm) * (y_true - ym)).mean())\n",
    "        a = cov / xvar\n",
    "        b = ym - a * xm\n",
    "    return a, b, a * y_pred + b\n",
    "\n",
    "def calibration_report(y_true, y_pred, name=\"PC\"):\n",
    "    p0 = pearson(y_true, y_pred); r0 = rmse(y_true, y_pred)\n",
    "    std_ratio = float(np.std(y_pred) / np.std(y_true)) if np.std(y_true) > 0 else np.nan\n",
    "    bias = float(y_pred.mean() - y_true.mean())\n",
    "\n",
    "    a, b, y_hat = fit_affine(y_true, y_pred)\n",
    "    p1 = pearson(y_true, y_hat); r1 = rmse(y_true, y_hat)\n",
    "\n",
    "    print(f\"\\n{name} calibration\")\n",
    "    print(f\"  Pearson:  {p0:.5f}  ->  {p1:.5f}\")\n",
    "    print(f\"  RMSE:     {r0:.5f}  ->  {r1:.5f}\")\n",
    "    print(f\"  std_ratio(pred/true): {std_ratio:.3f}\")\n",
    "    print(f\"  bias(pred-true mean): {bias:.5f}\")\n",
    "    print(f\"  affine: a={a:.5f}, b={b:.5f}\")\n",
    "\n",
    "calibration_report(y_va, pred, name=f\"PC{j+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f266b48-a185-4199-a85c-a3cad29ce84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cols: 16582\n",
      "PCA cols:   [0, 1000)\n",
      "Peaks cols: [1000, 16000)\n",
      "Meta cols:  [16000, 16582)  (N_META=582)\n",
      "PC panel:   [1, 4, 9, 29]\n",
      "\n",
      "=== Running feature ablation (Candidate A) ===\n",
      "\n",
      "-- PCA_only -- shape=(20000, 1582), nnz=20,288,764, density=0.6412\n",
      "PCA_only: mean Pearson over PCs = 0.76387 | time=3.9 min\n",
      "  PC  2: pearson=0.88290 | best_iter= 175 | rmse=13.9719\n",
      "  PC  5: pearson=0.91093 | best_iter= 432 | rmse=6.1703\n",
      "  PC 10: pearson=0.71369 | best_iter= 268 | rmse=9.5020\n",
      "  PC 30: pearson=0.54797 | best_iter= 273 | rmse=3.4331\n",
      "\n",
      "-- Peaks_only -- shape=(20000, 15582), nnz=9,078,947, density=0.0291\n",
      "Peaks_only: mean Pearson over PCs = 0.49625 | time=10.4 min\n",
      "  PC  2: pearson=0.77826 | best_iter= 795 | rmse=17.5165\n",
      "  PC  5: pearson=0.69959 | best_iter= 799 | rmse=10.6508\n",
      "  PC 10: pearson=0.42399 | best_iter= 386 | rmse=11.1888\n",
      "  PC 30: pearson=0.08314 | best_iter= 124 | rmse=4.0412\n",
      "\n",
      "-- Full -- shape=(20000, 16582), nnz=29,078,947, density=0.0877\n",
      "Full: mean Pearson over PCs = 0.76225 | time=12.4 min\n",
      "  PC  2: pearson=0.88177 | best_iter= 189 | rmse=14.1190\n",
      "  PC  5: pearson=0.90856 | best_iter= 540 | rmse=6.2604\n",
      "  PC 10: pearson=0.70646 | best_iter= 194 | rmse=9.4784\n",
      "  PC 30: pearson=0.55221 | best_iter= 208 | rmse=3.4288\n",
      "\n",
      "=== Ablation summary (higher mean Pearson is better) ===\n",
      "  PCA_only: meanPearson=0.76387 | time=3.9 min\n",
      "      Full: meanPearson=0.76225 | time=12.4 min\n",
      "Peaks_only: meanPearson=0.49625 | time=10.4 min\n",
      "\n",
      "Winner: PCA_only\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Feature Ablation: PCA-only vs Peaks-only vs Full (same model)\n",
    "# Efficient + laptop-friendly:\n",
    "# - float32 CSR (+ int32 indices)\n",
    "# - row subsample\n",
    "# - small PC panel\n",
    "# - Candidate A fixed\n",
    "# - evaluates mean Pearson over CELLS per PC\n",
    "# ============================================================\n",
    "\n",
    "import time, gc\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (edit if needed)\n",
    "# -----------------------------\n",
    "SEED = 77\n",
    "N_JOBS = 8\n",
    "MAX_BIN = 128\n",
    "NUM_BOOST_ROUND = 800\n",
    "EARLY_STOP = 30\n",
    "\n",
    "TRAIN_SUB = 20000\n",
    "VAL_SUB   = 12000\n",
    "\n",
    "# PCs to evaluate (0-based). Keep small and spread out.\n",
    "PC_PANEL = [1, 4, 9, 29]   # PCs 2,5,10,30\n",
    "\n",
    "# ---- Column layout in your x_train ----\n",
    "# Based on your description: 1000 PCA + 15000 peaks + day (1 col)\n",
    "N_PCA   = 1000\n",
    "N_PEAKS = 15000\n",
    "# If you added more than one \"day\" feature, set N_META accordingly.\n",
    "N_META  = None  # auto: total_cols - (N_PCA + N_PEAKS)\n",
    "\n",
    "# Candidate A (locked)\n",
    "cand_A = dict(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=20,\n",
    "    colsample_bytree=0.30,\n",
    "    subsample=0.80,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=1.0,\n",
    ")\n",
    "\n",
    "BASE_PARAMS = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    tree_method=\"hist\",\n",
    "    max_bin=MAX_BIN,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbosity=0,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def pearson_corr(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    b = np.asarray(b, dtype=np.float32)\n",
    "    a = a - a.mean()\n",
    "    b = b - b.mean()\n",
    "    denom = float(np.sqrt((a*a).sum()) * np.sqrt((b*b).sum()))\n",
    "    if denom == 0.0:\n",
    "        return np.nan\n",
    "    return float((a*b).sum() / denom)\n",
    "\n",
    "def eval_one_ablation(X_tr, X_va, Y_tr, Y_va, pc_panel, params,\n",
    "                      num_boost_round=NUM_BOOST_ROUND, early_stop=EARLY_STOP):\n",
    "    \"\"\"\n",
    "    For fixed X_tr/X_va and multiple PCs:\n",
    "      - build DMatrix once\n",
    "      - swap label via set_float_info\n",
    "      - train each PC\n",
    "      - return per-PC pearson + mean pearson\n",
    "    \"\"\"\n",
    "    pc0 = pc_panel[0]\n",
    "    dtrain = xgb.DMatrix(X_tr, label=Y_tr[:, pc0])\n",
    "    dvalid = xgb.DMatrix(X_va, label=Y_va[:, pc0])\n",
    "\n",
    "    per_pc = []\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    for j in pc_panel:\n",
    "        ytr = Y_tr[:, j]\n",
    "        yva = Y_va[:, j]\n",
    "        dtrain.set_float_info(\"label\", ytr)\n",
    "        dvalid.set_float_info(\"label\", yva)\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            evals=[(dvalid, \"val\")],\n",
    "            early_stopping_rounds=early_stop,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration + 1))\n",
    "        pr = pearson_corr(yva, pred)\n",
    "        per_pc.append((j, pr, int(booster.best_iteration), float(booster.best_score)))\n",
    "\n",
    "        del booster, pred\n",
    "        gc.collect()\n",
    "\n",
    "    total_s = time.perf_counter() - t0\n",
    "    mean_pr = float(np.nanmean([x[1] for x in per_pc]))\n",
    "\n",
    "    del dtrain, dvalid\n",
    "    gc.collect()\n",
    "\n",
    "    return mean_pr, per_pc, total_s\n",
    "\n",
    "# -----------------------------\n",
    "# PREP DATA (float32 CSR + subsample)\n",
    "# -----------------------------\n",
    "assert sp.isspmatrix_csr(x_train), \"Expected x_train to be CSR\"\n",
    "x_train32 = x_train.astype(np.float32, copy=False)\n",
    "x_train32.indices = x_train32.indices.astype(np.int32, copy=False)\n",
    "x_train32.indptr  = x_train32.indptr.astype(np.int32, copy=False)\n",
    "\n",
    "y_train32 = np.asarray(y_train, dtype=np.float32, order=\"C\")\n",
    "n_targets = y_train32.shape[1]\n",
    "\n",
    "pc_panel = [j for j in PC_PANEL if 0 <= j < n_targets]\n",
    "if not pc_panel:\n",
    "    raise ValueError(f\"PC_PANEL {PC_PANEL} out of range for n_targets={n_targets}\")\n",
    "\n",
    "n_cols = x_train32.shape[1]\n",
    "if N_META is None:\n",
    "    N_META = n_cols - (N_PCA + N_PEAKS)\n",
    "if N_META < 0:\n",
    "    raise ValueError(\n",
    "        f\"Column split invalid: total_cols={n_cols}, N_PCA={N_PCA}, N_PEAKS={N_PEAKS} \"\n",
    "        f\"implies N_META={N_META} < 0. Fix N_PCA/N_PEAKS.\"\n",
    "    )\n",
    "\n",
    "pca_cols   = slice(0, N_PCA)\n",
    "peaks_cols = slice(N_PCA, N_PCA + N_PEAKS)\n",
    "meta_cols  = slice(N_PCA + N_PEAKS, N_PCA + N_PEAKS + N_META)\n",
    "\n",
    "print(f\"Total cols: {n_cols}\")\n",
    "print(f\"PCA cols:   [0, {N_PCA})\")\n",
    "print(f\"Peaks cols: [{N_PCA}, {N_PCA+N_PEAKS})\")\n",
    "print(f\"Meta cols:  [{N_PCA+N_PEAKS}, {N_PCA+N_PEAKS+N_META})  (N_META={N_META})\")\n",
    "print(f\"PC panel:   {pc_panel}\")\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "tr_idx = np.where(tr_mask)[0]\n",
    "va_idx = np.where(va_mask)[0]\n",
    "tr_sub = rng.choice(tr_idx, size=min(TRAIN_SUB, tr_idx.size), replace=False)\n",
    "va_sub = rng.choice(va_idx, size=min(VAL_SUB,   va_idx.size), replace=False)\n",
    "\n",
    "Y_tr = y_train32[tr_sub]\n",
    "Y_va = y_train32[va_sub]\n",
    "\n",
    "# Pre-slice rows once\n",
    "X_tr_full = x_train32[tr_sub]\n",
    "X_va_full = x_train32[va_sub]\n",
    "\n",
    "# Build ablation matrices (still CSR)\n",
    "X_tr_pca   = X_tr_full[:, pca_cols]\n",
    "X_va_pca   = X_va_full[:, pca_cols]\n",
    "\n",
    "X_tr_peaks = X_tr_full[:, peaks_cols]\n",
    "X_va_peaks = X_va_full[:, peaks_cols]\n",
    "\n",
    "# Full = PCA + Peaks (+ meta if present)\n",
    "# If meta exists, include it consistently across all experiments.\n",
    "# (Day can matter; ablation is about PCA vs peaks, not about dropping meta.)\n",
    "if N_META > 0:\n",
    "    X_tr_meta = X_tr_full[:, meta_cols]\n",
    "    X_va_meta = X_va_full[:, meta_cols]\n",
    "\n",
    "    X_tr_pca_m   = sp.hstack([X_tr_pca,   X_tr_meta], format=\"csr\")\n",
    "    X_va_pca_m   = sp.hstack([X_va_pca,   X_va_meta], format=\"csr\")\n",
    "\n",
    "    X_tr_peaks_m = sp.hstack([X_tr_peaks, X_tr_meta], format=\"csr\")\n",
    "    X_va_peaks_m = sp.hstack([X_va_peaks, X_va_meta], format=\"csr\")\n",
    "\n",
    "    X_tr_full_m  = X_tr_full\n",
    "    X_va_full_m  = X_va_full\n",
    "else:\n",
    "    X_tr_pca_m, X_va_pca_m = X_tr_pca, X_va_pca\n",
    "    X_tr_peaks_m, X_va_peaks_m = X_tr_peaks, X_va_peaks\n",
    "    X_tr_full_m, X_va_full_m = X_tr_full, X_va_full\n",
    "\n",
    "paramsA = {**BASE_PARAMS, **cand_A}\n",
    "\n",
    "# -----------------------------\n",
    "# RUN ABLATIONS\n",
    "# -----------------------------\n",
    "ablations = [\n",
    "    (\"PCA_only\",   X_tr_pca_m,   X_va_pca_m),\n",
    "    (\"Peaks_only\", X_tr_peaks_m, X_va_peaks_m),\n",
    "    (\"Full\",       X_tr_full_m,  X_va_full_m),\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(\"\\n=== Running feature ablation (Candidate A) ===\")\n",
    "for name, Xtr, Xva in ablations:\n",
    "    nnz = Xtr.nnz\n",
    "    dens = nnz / (Xtr.shape[0] * Xtr.shape[1])\n",
    "    print(f\"\\n-- {name} -- shape={Xtr.shape}, nnz={nnz:,}, density={dens:.4f}\")\n",
    "\n",
    "    mean_pr, per_pc, total_s = eval_one_ablation(Xtr, Xva, Y_tr, Y_va, pc_panel, paramsA)\n",
    "    results.append((name, mean_pr, per_pc, total_s))\n",
    "\n",
    "    print(f\"{name}: mean Pearson over PCs = {mean_pr:.5f} | time={total_s/60:.1f} min\")\n",
    "    for (j, pr, best_it, best_rmse) in per_pc:\n",
    "        print(f\"  PC{j+1:>3}: pearson={pr:.5f} | best_iter={best_it:>4} | rmse={best_rmse:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# SUMMARY\n",
    "# -----------------------------\n",
    "print(\"\\n=== Ablation summary (higher mean Pearson is better) ===\")\n",
    "results_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "for name, mean_pr, per_pc, total_s in results_sorted:\n",
    "    print(f\"{name:>10}: meanPearson={mean_pr:.5f} | time={total_s/60:.1f} min\")\n",
    "\n",
    "winner = results_sorted[0]\n",
    "print(f\"\\nWinner: {winner[0]}\")\n",
    "\n",
    "# Cleanup big objects\n",
    "del X_tr_full, X_va_full, X_tr_pca, X_va_pca, X_tr_peaks, X_va_peaks\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124f9a9-c9cd-4952-9104-4be3066ec4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost Helper Functions ---\n",
    "\n",
    "def kaggle_mean_cellwise_pearson(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mean Pearson correlation per cell (row-wise).\n",
    "    Mimics the competition metric. Constant predictions yield -1.\n",
    "    \"\"\"\n",
    "    yt = np.asarray(y_true)\n",
    "    yp = np.asarray(y_pred)\n",
    "    \n",
    "    # Center data (subtract mean)\n",
    "    y_pred_centered = yp - yp.mean(axis=1, keepdims=True)\n",
    "    y_true_centered = yt - yt.mean(axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate numerator and denominator\n",
    "    num = np.sum(y_true_centered * y_pred_centered, axis=1)\n",
    "    den = np.sqrt(np.sum(y_true_centered ** 2, axis=1) * np.sum(y_pred_centered ** 2, axis=1))\n",
    "    \n",
    "    # Handle constant predictions (zero variance)\n",
    "    const_pred = np.all(yp == yp[:, [0]], axis=1)\n",
    "    \n",
    "    corrs = np.empty(y_true.shape[0], dtype=float)\n",
    "    corrs[const_pred] = -1.0  # Penalize constant predictions\n",
    "    \n",
    "    # Calculate valid correlations\n",
    "    valid = (~const_pred) & (den > 0)\n",
    "    corrs[valid] = num[valid] / den[valid]\n",
    "    \n",
    "    # Handle remaining edge cases\n",
    "    corrs[~const_pred & (den == 0)] = np.nan\n",
    "    \n",
    "    return np.nanmean(corrs)\n",
    "\n",
    "def split_indices_donor(X_df, donor_col=\"donor\", random_state=77):\n",
    "    \"\"\"\n",
    "    Splits data by holding out ONE donor for validation.\n",
    "    Crucial for testing generalization to new batches/donors.\n",
    "    \"\"\"\n",
    "    donors = X_df[donor_col].unique()\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    val_donor = rng.choice(donors)\n",
    "    \n",
    "    train_idx = np.where(X_df[donor_col] != val_donor)[0]\n",
    "    val_idx   = np.where(X_df[donor_col] == val_donor)[0]\n",
    "    \n",
    "    return train_idx, val_idx, val_donor\n",
    "\n",
    "def to_float32(df):\n",
    "    \"\"\"Converts DataFrame to float32 numpy array (XGBoost friendly).\"\"\"\n",
    "    return df.to_numpy().astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "994c9449-319f-4c1a-9845-ff3d544dcba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['56390cf1b95e' 'fc0c60183c33' '9b4a87e22ad0' '81cccad8cd81'\n",
      " '15cb3d85c232' 'a7791bcf1152' '072790e768b1' '404459b1005b'\n",
      " '627a5071cbd7' '00f283126092' '627703f5faa0' '3894c8880096'\n",
      " 'e0af51ad3900' 'e31ca103a4ac' '47711761153f' 'c9d7ec67e230'\n",
      " '4495e228dcbd' '89c1d660a925' '7d66e9fac697' '73e80a80ac36']\n",
      "X= 500 Y=100  Pearson=0.66602 Â± 0.00015\n",
      "X= 500 Y=200  Pearson=0.66599 Â± 0.00015\n",
      "X= 500 Y=300  Pearson=0.66595 Â± 0.00015\n",
      "X= 800 Y=100  Pearson=0.66601 Â± 0.00015\n",
      "X= 800 Y=200  Pearson=0.66595 Â± 0.00015\n",
      "X= 800 Y=300  Pearson=0.66589 Â± 0.00015\n",
      "X=1000 Y=100  Pearson=0.66599 Â± 0.00015\n",
      "X=1000 Y=200  Pearson=0.66591 Â± 0.00015\n",
      "X=1000 Y=300  Pearson=0.66584 Â± 0.00015\n"
     ]
    }
   ],
   "source": [
    "print(valid_ids[:20])\n",
    "\n",
    "def kaggle_mean_cellwise_pearson(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean per-cell Pearson correlation with the rule that constant prediction \n",
    "    should give -1 in correlation\n",
    "    \"\"\"\n",
    "    yt = np.asarray(y_true)\n",
    "    yp = np.asarray(y_pred)\n",
    "    # constant prediction per row -> -1\n",
    "    const_pred = np.all(yp == yp[:, [0]], axis=1)\n",
    "    y_pred_centered = yp - yp.mean(axis=1, keepdims=True)\n",
    "    y_true_centered = yt - yt.mean(axis=1, keepdims=True)\n",
    "    num = np.sum(y_true_centered * y_pred_centered, axis=1)\n",
    "    den = np.sqrt(np.sum(y_true_centered **2, axis=1) * np.sum(y_pred_centered**2, axis=1))\n",
    "    corrs = np.empty(y_true.shape[0], dtype=float)\n",
    "    corrs[const_pred] = -1.0\n",
    "    # Pearson can be calculated\n",
    "    valid = (~const_pred) & (den > 0)\n",
    "    corrs[valid] = num[valid] / den[valid]\n",
    "    # non-constant with zero denominator\n",
    "    corrs[~const_pred & (den == 0)] = np.nan\n",
    "    return np.nanmean(corrs)  \n",
    "    \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "# --- FIRST CHANGE: load what you need to score in gene space ---\n",
    "Y_pca_components = np.load(\"Y_ipca_components_300.npy\")   # shape (300, n_genes)\n",
    "Y_pca_mean       = np.load(\"Y_ipca_mean.npy\")             # shape (n_genes,)\n",
    "\n",
    "# Y_full must be the true RNA in gene space (n_cells, n_genes)\n",
    "# If you don't already have it as an array/memmap, load it from the h5 (this will be slow if done repeatedly).\n",
    "import tables\n",
    "with tables.open_file(\"train_multi_targets.h5\", \"r\") as f:\n",
    "    Y_full = f.get_node(\"/train_multi_targets/block0_values\")[:]   # shape (n_cells, n_genes)\n",
    "\n",
    "X_dims = [500, 800, 1000]\n",
    "Y_dims = [100, 200, 300]\n",
    "\n",
    "groups = (np.array([hash(s) for s in valid_ids])\n",
    "          & 0x7fffffffffffffff) % 50\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "\n",
    "for xd in X_dims:\n",
    "    X = X_csr_1000[:, :xd]\n",
    "    for yd in Y_dims:\n",
    "        Y = Y_pca[:, :yd]\n",
    "\n",
    "        scores = []\n",
    "        for tr, va in gss.split(X, Y, groups):\n",
    "            model = Ridge(alpha=1.0)\n",
    "            model.fit(X[tr], Y[tr])\n",
    "            pred = model.predict(X[va])                  # (n_val, yd)\n",
    "            W = Y_pca_components[:yd, :]                 # (yd, n_genes)\n",
    "            Y_pred_full = pred @ W + Y_pca_mean[None, :] # (n_val, n_genes)\n",
    "            Y_true_full = Y_full[va]\n",
    "\n",
    "            score = kaggle_mean_cellwise_pearson(Y_true_full, Y_pred_full)\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        print(\n",
    "            f\"X={xd:4d} Y={yd:3d}  \"\n",
    "            f\"Pearson={np.mean(scores):.5f} Â± {np.std(scores):.5f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72730237-c3e3-48be-aa2e-08d7871d998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Input detected as Strings. Mapping to indices with validation...\n",
      "âœ… Found 1376144 of 9633008 requested genes.\n",
      "âš ï¸ 8256864 genes were not found in the dataset and will be skipped.\n",
      "Evaluation locked to 1376144 genes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load and Standardize Gene Names\n",
    "all_gene_names = np.load(\"train_multi_genes.npy\", allow_pickle=True)\n",
    "if all_gene_names.dtype.kind == 'S':\n",
    "    all_gene_names = np.char.decode(all_gene_names)\n",
    "all_gene_names = all_gene_names.astype(str)\n",
    "\n",
    "# 2. Standardize target_cols\n",
    "if isinstance(target_cols, (pd.DataFrame, pd.Series)):\n",
    "    target_cols = target_cols.values.flatten()\n",
    "target_cols = np.asarray(target_cols)\n",
    "\n",
    "# 3. Robust Logic\n",
    "is_numeric = np.issubdtype(target_cols.dtype, np.number)\n",
    "\n",
    "if is_numeric:\n",
    "    print(f\"âœ… Input detected as Integers. Checking bounds...\")\n",
    "    # Filter out any indices that are too large\n",
    "    valid_mask = target_cols < len(all_gene_names)\n",
    "    target_indices = target_cols[valid_mask].astype(int)\n",
    "    \n",
    "    if len(target_indices) < len(target_cols):\n",
    "        print(f\"âš ï¸ Warning: Dropped {len(target_cols) - len(target_indices)} indices that were out of bounds.\")\n",
    "\n",
    "else:\n",
    "    print(f\"ðŸ” Input detected as Strings. Mapping to indices with validation...\")\n",
    "    target_cols_str = target_cols.astype(str)\n",
    "    \n",
    "    # Sort for fast searching\n",
    "    sorter = np.argsort(all_gene_names)\n",
    "    \n",
    "    # Get insertion points\n",
    "    insertion_idx = np.searchsorted(all_gene_names, target_cols_str, sorter=sorter)\n",
    "    \n",
    "    # Identify which ones are actually valid\n",
    "    # 1. Must be within bounds (index < length)\n",
    "    # 2. The value at that index must actually MATCH the target name\n",
    "    is_in_bounds = insertion_idx < len(all_gene_names)\n",
    "    \n",
    "    # Check for exact matches\n",
    "    # We only check the ones that are in bounds to avoid the IndexError you just saw\n",
    "    valid_mask = is_in_bounds.copy()\n",
    "    valid_mask[is_in_bounds] &= (all_gene_names[sorter[insertion_idx[is_in_bounds]]] == target_cols_str[is_in_bounds])\n",
    "    \n",
    "    # Select the valid indices\n",
    "    target_indices = sorter[insertion_idx[valid_mask]]\n",
    "    \n",
    "    print(f\"âœ… Found {len(target_indices)} of {len(target_cols)} requested genes.\")\n",
    "    if len(target_indices) < len(target_cols):\n",
    "        print(f\"âš ï¸ {len(target_cols) - len(target_indices)} genes were not found in the dataset and will be skipped.\")\n",
    "\n",
    "print(f\"Evaluation locked to {len(target_indices)} genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c35b9a-966e-4370-982f-2b0755e64c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Validation Targets into RAM...\n",
      "âœ… Targets Loaded. Shape: (105942, 23418)\n",
      "\n",
      "ðŸš€ Training: X=1000, Y=100\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'groups' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     35\u001b[39m Y_mean_sliced = Y_mean[target_indices]\n\u001b[32m     37\u001b[39m scores = []\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split_i, (tr, va) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gss.split(X_csr_1000, Y_pca, \u001b[43mgroups\u001b[49m)):\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     41\u001b[39m     model = Ridge(alpha=\u001b[32m1.0\u001b[39m)\n\u001b[32m     42\u001b[39m     model.fit(X_feat[tr], Y_pca[tr, :yd])\n",
      "\u001b[31mNameError\u001b[39m: name 'groups' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# 1. Load the TRUE RNA values into RAM (It's only ~9 GB, you have 62 GB)\n",
    "# This makes everything much faster than reading from disk\n",
    "import tables\n",
    "with tables.open_file(\"train_multi_targets.h5\", \"r\") as f:\n",
    "    # Use the target_indices immediately to only load what we need\n",
    "    # If target_indices is still broken, we default to all genes\n",
    "    if len(target_indices) > 30000: \n",
    "        target_indices = np.arange(23418)\n",
    "        \n",
    "    print(\"Loading Validation Targets into RAM...\")\n",
    "    Y_full = f.get_node(\"/train_multi_targets/block0_values\")[:]\n",
    "    # Filter Y_full to only the genes we care about right now\n",
    "    Y_full = Y_full[:, target_indices] \n",
    "\n",
    "print(f\"âœ… Targets Loaded. Shape: {Y_full.shape}\")\n",
    "\n",
    "# 2. Training Loop\n",
    "X_dims = [1000]\n",
    "Y_dims = [100, 300] \n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc2c7f56-6deb-465d-9cb9-75c994b06bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Groups defined. Shape: (105942,)\n",
      "\n",
      "ðŸš€ Training: X=1000, Y=100\n",
      "  Split 1: 0.66596\n",
      "  Split 2: 0.66644\n",
      "âœ… Result: X=1000 Y=100 | Pearson=0.66620\n",
      "\n",
      "ðŸš€ Training: X=1000, Y=300\n",
      "  Split 1: 0.66582\n",
      "  Split 2: 0.66629\n",
      "âœ… Result: X=1000 Y=300 | Pearson=0.66605\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Load the Cell IDs (saved from Notebook 1)\n",
    "cell_ids_bytes = np.load(\"train_multi_cell_ids.npy\")\n",
    "\n",
    "# 2. Decode them to strings (required for consistent hashing)\n",
    "if cell_ids_bytes.dtype.kind == 'S':\n",
    "    valid_ids = np.char.decode(cell_ids_bytes)\n",
    "else:\n",
    "    valid_ids = cell_ids_bytes.astype(str)\n",
    "\n",
    "# 3. Create the 'groups' variable\n",
    "# We use hashing to assign arbitrary groups if donor info isn't explicitly loaded\n",
    "groups = (np.array([hash(s) for s in valid_ids]) & 0x7fffffffffffffff) % 50\n",
    "\n",
    "print(f\"âœ… Groups defined. Shape: {groups.shape}\")\n",
    "for xd in X_dims:\n",
    "    X_feat = X_csr_1000[:, :xd]\n",
    "    \n",
    "    for yd in Y_dims:\n",
    "        print(f\"\\nðŸš€ Training: X={xd}, Y={yd}\")\n",
    "        \n",
    "        # Slice Decoder Weights for the specific targets we are using\n",
    "        W_sliced = Y_components[:yd, target_indices]\n",
    "        Y_mean_sliced = Y_mean[target_indices]\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for split_i, (tr, va) in enumerate(gss.split(X_csr_1000, Y_pca, groups)):\n",
    "            # Train\n",
    "            model = Ridge(alpha=1.0)\n",
    "            model.fit(X_feat[tr], Y_pca[tr, :yd])\n",
    "            \n",
    "            # Predict (Latent)\n",
    "            pred_latent = model.predict(X_feat[va])\n",
    "            \n",
    "            # Decode (Gene Space)\n",
    "            pred_gene = pred_latent @ W_sliced + Y_mean_sliced\n",
    "            \n",
    "            # Score\n",
    "            # Y_full is already filtered and in memory, so we just slice rows [va]\n",
    "            true_gene = Y_full[va]\n",
    "            \n",
    "            # Pearson Calculation (Vectorized)\n",
    "            # Centering\n",
    "            p_centered = pred_gene - pred_gene.mean(axis=1, keepdims=True)\n",
    "            t_centered = true_gene - true_gene.mean(axis=1, keepdims=True)\n",
    "            # Correlation\n",
    "            num = np.sum(p_centered * t_centered, axis=1)\n",
    "            den = np.sqrt(np.sum(p_centered**2, axis=1) * np.sum(t_centered**2, axis=1))\n",
    "            corr = num / den\n",
    "            corr[den == 0] = 0\n",
    "            \n",
    "            mean_score = np.nanmean(corr)\n",
    "            scores.append(mean_score)\n",
    "            print(f\"  Split {split_i+1}: {mean_score:.5f}\")\n",
    "            \n",
    "            del model, pred_latent, pred_gene, corr\n",
    "            gc.collect()\n",
    "            \n",
    "        print(f\"âœ… Result: X={xd} Y={yd} | Pearson={np.mean(scores):.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
